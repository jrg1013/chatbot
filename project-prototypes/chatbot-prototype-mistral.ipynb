{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa125089",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T10:03:31.238220Z",
     "iopub.status.busy": "2023-11-01T10:03:31.237478Z",
     "iopub.status.idle": "2023-11-01T10:03:31.252839Z",
     "shell.execute_reply": "2023-11-01T10:03:31.251578Z",
     "shell.execute_reply.started": "2023-11-01T10:03:31.238189Z"
    },
    "papermill": {
     "duration": 0.006004,
     "end_time": "2023-11-01T12:18:03.653845",
     "exception": false,
     "start_time": "2023-11-01T12:18:03.647841",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introducción\n",
    "\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "Utilizar Llama 2.0, Langchain y ChromaDB para crear un sistema de Generación con Recuperación Mejorada (RAG). Esto nos permitirá hacer preguntas sobre nuestros documentos (que no se incluyeron en los datos de entrenamiento), sin necesidad de ajustar finamente el Modelo de Lenguaje Grande (LLM, por sus siglas en inglés).\n",
    "Cuando se utiliza RAG, si se plantea una pregunta, primero se realiza un paso de recuperación para obtener documentos relevantes de una base de datos especial, una base de datos vectorial donde se indexaron estos documentos.\n",
    "\n",
    "## Definiciones\n",
    "\n",
    "* LLM - Modelo de Lenguaje Grande (Large Language Model)\n",
    "* Llama 2.0 - LLM de Meta\n",
    "* Langchain - un marco diseñado para simplificar la creación de aplicaciones utilizando LLM\n",
    "* Base de datos vectorial - una base de datos que organiza datos a través de vectores de alta dimensión\n",
    "* ChromaDB - base de datos vectorial\n",
    "* RAG - Generación con Recuperación Mejorada (consulte más detalles sobre RAG a continuación)\n",
    "\n",
    "## Detalles del modelo\n",
    "\n",
    "* **Modelo**:  Mistral\n",
    "* **Variante**: 7b-v0.1-hf (7b: 7B dimm. hf: HuggingFace build)\n",
    "* **Versión**: V1\n",
    "* **Framework**: PyTorch\n",
    "\n",
    "\n",
    "\n",
    "## ¿Qué es un sistema de Generación con Recuperación Mejorada (RAG)?\n",
    "\n",
    "Los Modelos de Lenguaje Grande (LLM) han demostrado su capacidad para comprender el contexto y proporcionar respuestas precisas a diversas tareas de Procesamiento de Lenguaje Natural (NLP), incluyendo la resumen, preguntas y respuestas, cuando se les solicita. Si bien son capaces de proporcionar respuestas muy buenas a preguntas sobre información con la que fueron entrenados, tienden a alucinar cuando el tema trata sobre información que \"no saben\", es decir, no estaba incluida en sus datos de entrenamiento. La Generación con Recuperación Mejorada combina recursos externos con LLM. Por lo tanto, los dos componentes principales de un sistema RAG son un recuperador y un generador.\n",
    "\n",
    "La parte del recuperador se puede describir como un sistema que es capaz de codificar nuestros datos para que se puedan recuperar fácilmente las partes relevantes al consultarlos. La codificación se realiza utilizando incrustaciones de texto, es decir, un modelo entrenado para crear una representación vectorial de la información. La mejor opción para implementar un recuperador es una base de datos vectoriales. Como bases de datos vectoriales, existen múltiples opciones, tanto productos de código abierto como comerciales. Algunos ejemplos son ChromaDB, Mevius, FAISS, Pinecone, Weaviate. Nuestra opción en este cuaderno será una instancia local de ChromaDB (persistente).\n",
    "\n",
    "Para la parte del generador, la opción más obvia es un LLM. En este cuaderno utilizaremos un modelo LLaMA v2 cuantificado, de la colección de Modelos de Kaggle.\n",
    "\n",
    "La orquestación del recuperador y el generador se realizará utilizando Langchain. Una función especializada de Langchain nos permite crear el recuperador-generador en una sola línea de código.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35370c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T10:03:28.813845Z",
     "iopub.status.busy": "2023-11-01T10:03:28.813422Z",
     "iopub.status.idle": "2023-11-01T10:03:28.818410Z",
     "shell.execute_reply": "2023-11-01T10:03:28.817336Z",
     "shell.execute_reply.started": "2023-11-01T10:03:28.813798Z"
    },
    "papermill": {
     "duration": 0.004265,
     "end_time": "2023-11-01T12:18:03.662982",
     "exception": false,
     "start_time": "2023-11-01T12:18:03.658717",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Installations, imports, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbf4b56e",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-11-01T12:18:03.673805Z",
     "iopub.status.busy": "2023-11-01T12:18:03.672900Z",
     "iopub.status.idle": "2023-11-01T12:18:58.803762Z",
     "shell.execute_reply": "2023-11-01T12:18:58.802781Z"
    },
    "papermill": {
     "duration": 55.138637,
     "end_time": "2023-11-01T12:18:58.806126",
     "exception": false,
     "start_time": "2023-11-01T12:18:03.667489",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.33.0)\r\n",
      "Collecting transformers\r\n",
      "  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.16.4)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.6.3)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\r\n",
      "Collecting tokenizers<0.15,>=0.14 (from transformers)\r\n",
      "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.3)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.9.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.6.3)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\r\n",
      "Installing collected packages: tokenizers, transformers\r\n",
      "  Attempting uninstall: tokenizers\r\n",
      "    Found existing installation: tokenizers 0.13.3\r\n",
      "    Uninstalling tokenizers-0.13.3:\r\n",
      "      Successfully uninstalled tokenizers-0.13.3\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.33.0\r\n",
      "    Uninstalling transformers-4.33.0:\r\n",
      "      Successfully uninstalled transformers-4.33.0\r\n",
      "Successfully installed tokenizers-0.14.1 transformers-4.34.1\r\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.22.0)\r\n",
      "Collecting accelerate\r\n",
      "  Downloading accelerate-0.24.1-py3-none-any.whl (261 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.23.5)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0)\r\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.0.0)\r\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.16.4)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.0.9)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.12.2)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.6.3)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2023.9.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.1.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\r\n",
      "Installing collected packages: accelerate\r\n",
      "  Attempting uninstall: accelerate\r\n",
      "    Found existing installation: accelerate 0.22.0\r\n",
      "    Uninstalling accelerate-0.22.0:\r\n",
      "      Successfully uninstalled accelerate-0.22.0\r\n",
      "Successfully installed accelerate-0.24.1\r\n",
      "Collecting bitsandbytes\r\n",
      "  Downloading bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\r\n",
      "Successfully installed bitsandbytes-0.41.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers\n",
    "!pip install -U accelerate\n",
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c2f2a32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T12:18:58.826935Z",
     "iopub.status.busy": "2023-11-01T12:18:58.826611Z",
     "iopub.status.idle": "2023-11-01T12:19:24.175082Z",
     "shell.execute_reply": "2023-11-01T12:19:24.173985Z"
    },
    "papermill": {
     "duration": 25.361653,
     "end_time": "2023-11-01T12:19:24.177605",
     "exception": false,
     "start_time": "2023-11-01T12:18:58.815952",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb87d73",
   "metadata": {
    "papermill": {
     "duration": 0.010384,
     "end_time": "2023-11-01T12:19:24.200132",
     "exception": false,
     "start_time": "2023-11-01T12:19:24.189748",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inicializar modelo, tokenizador, y canal de consultas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78943a5",
   "metadata": {
    "papermill": {
     "duration": 0.010174,
     "end_time": "2023-11-01T12:19:24.220686",
     "exception": false,
     "start_time": "2023-11-01T12:19:24.210512",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "Preparar el modelo y el tokenizador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a385279",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-11-01T12:19:24.243524Z",
     "iopub.status.busy": "2023-11-01T12:19:24.242911Z",
     "iopub.status.idle": "2023-11-01T12:22:03.463882Z",
     "shell.execute_reply": "2023-11-01T12:22:03.462849Z"
    },
    "papermill": {
     "duration": 159.234784,
     "end_time": "2023-11-01T12:22:03.466022",
     "exception": false,
     "start_time": "2023-11-01T12:19:24.231238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc6606039e0248769430e5073522610f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare model, tokenizer: 159.215 sec.\n"
     ]
    }
   ],
   "source": [
    "time_1 = time()\n",
    "\n",
    "\n",
    "model_name = '/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "time_2 = time()\n",
    "print(f\"Prepare model, tokenizer: {round(time_2-time_1, 3)} sec.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339ae069",
   "metadata": {
    "papermill": {
     "duration": 0.009233,
     "end_time": "2023-11-01T12:22:03.484955",
     "exception": false,
     "start_time": "2023-11-01T12:22:03.475722",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Definir el query pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c9f3c9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T12:22:03.504942Z",
     "iopub.status.busy": "2023-11-01T12:22:03.504669Z",
     "iopub.status.idle": "2023-11-01T12:22:03.510476Z",
     "shell.execute_reply": "2023-11-01T12:22:03.509599Z"
    },
    "papermill": {
     "duration": 0.017989,
     "end_time": "2023-11-01T12:22:03.512439",
     "exception": false,
     "start_time": "2023-11-01T12:22:03.494450",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare pipeline: 0.0 sec.\n"
     ]
    }
   ],
   "source": [
    "time_1 = time()\n",
    "query_pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",)\n",
    "time_2 = time()\n",
    "print(f\"Prepare pipeline: {round(time_2-time_1, 3)} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d0517c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T12:22:03.532693Z",
     "iopub.status.busy": "2023-11-01T12:22:03.532408Z",
     "iopub.status.idle": "2023-11-01T12:22:09.373339Z",
     "shell.execute_reply": "2023-11-01T12:22:09.372493Z"
    },
    "papermill": {
     "duration": 5.853812,
     "end_time": "2023-11-01T12:22:09.375707",
     "exception": false,
     "start_time": "2023-11-01T12:22:03.521895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1539: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Cual es la capital de Alemania?\"},\n",
    "]\n",
    "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "model_inputs = encodeds\n",
    "generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\n",
    "decoded = tokenizer.batch_decode(generated_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0023387f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T12:22:09.398221Z",
     "iopub.status.busy": "2023-11-01T12:22:09.397677Z",
     "iopub.status.idle": "2023-11-01T12:22:09.402420Z",
     "shell.execute_reply": "2023-11-01T12:22:09.401584Z"
    },
    "papermill": {
     "duration": 0.017987,
     "end_time": "2023-11-01T12:22:09.404590",
     "exception": false,
     "start_time": "2023-11-01T12:22:09.386603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] Cual es la capital de Alemania? [/INST] La capital de Alemania es Berlín.</s>\n"
     ]
    }
   ],
   "source": [
    "print(decoded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc988bfa",
   "metadata": {
    "papermill": {
     "duration": 0.010121,
     "end_time": "2023-11-01T12:22:09.424428",
     "exception": false,
     "start_time": "2023-11-01T12:22:09.414307",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Credit:\n",
    " - Adapted from https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf743dfb",
   "metadata": {
    "papermill": {
     "duration": 0.010002,
     "end_time": "2023-11-01T12:22:09.444379",
     "exception": false,
     "start_time": "2023-11-01T12:22:09.434377",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Testear la query pipeline\n",
    "\n",
    "Testeamos el pipeline con una query sobre..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59a47e35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T12:22:09.465544Z",
     "iopub.status.busy": "2023-11-01T12:22:09.465274Z",
     "iopub.status.idle": "2023-11-01T12:22:09.471497Z",
     "shell.execute_reply": "2023-11-01T12:22:09.470624Z"
    },
    "papermill": {
     "duration": 0.019255,
     "end_time": "2023-11-01T12:22:09.473568",
     "exception": false,
     "start_time": "2023-11-01T12:22:09.454313",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_model(tokenizer, pipeline, prompt_to_test):\n",
    "    \"\"\"\n",
    "    Perform a query\n",
    "    print the result\n",
    "    Args:\n",
    "        tokenizer: the tokenizer\n",
    "        pipeline: the pipeline\n",
    "        prompt_to_test: the prompt\n",
    "    Returns\n",
    "        None\n",
    "    \"\"\"\n",
    "    # adapted from https://huggingface.co/blog/llama2#using-transformers\n",
    "    time_1 = time()\n",
    "    sequences = pipeline(\n",
    "        prompt_to_test,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_length=200,)\n",
    "    time_2 = time()\n",
    "    print(f\"Test inference: {round(time_2-time_1, 3)} sec.\")\n",
    "    for seq in sequences:\n",
    "        print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2eed4979",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T12:22:09.494191Z",
     "iopub.status.busy": "2023-11-01T12:22:09.493920Z",
     "iopub.status.idle": "2023-11-01T12:22:11.618046Z",
     "shell.execute_reply": "2023-11-01T12:22:11.616903Z"
    },
    "papermill": {
     "duration": 2.137103,
     "end_time": "2023-11-01T12:22:11.620369",
     "exception": false,
     "start_time": "2023-11-01T12:22:09.483266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test inference: 2.119 sec.\n",
      "Result: Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words. Answer from your perspective.\n"
     ]
    }
   ],
   "source": [
    "test_model(tokenizer,\n",
    "           query_pipeline,\n",
    "           \"Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f1040de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T12:22:11.645276Z",
     "iopub.status.busy": "2023-11-01T12:22:11.644974Z",
     "iopub.status.idle": "2023-11-01T12:22:21.920499Z",
     "shell.execute_reply": "2023-11-01T12:22:21.919386Z"
    },
    "papermill": {
     "duration": 10.289729,
     "end_time": "2023-11-01T12:22:21.922569",
     "exception": false,
     "start_time": "2023-11-01T12:22:11.632840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test inference: 10.271 sec.\n",
      "Result: Explica que es el discurso sobre el estado de la nación en EE.UU. Hazlo en menos de 100 palabras.\n",
      "\n",
      "El estado de la nación en EE.UU está resaltando por la pandemia de COVID-19 y una crisis económica. En 2020 se registró una caída de un 3.5% en el PIB, y la tasa de desempleo alcanzó el 6% en diciembre. La pandemia ha llevado a un aumento en los casos de asistencia social y a una demanda en los medios de comunicación. El presidente Joe Biden ha propuesto una recuperación de $2 trilones para crear empleos y ayudar a las personas afligidas por la crisis económica.\n"
     ]
    }
   ],
   "source": [
    "test_model(tokenizer,\n",
    "           query_pipeline,\n",
    "           \"Explica que es el discurso sobre el estado de la nación en EE.UU. Hazlo en menos de 100 palabras.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 266.432024,
   "end_time": "2023-11-01T12:22:25.460143",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-01T12:17:59.028119",
   "version": "2.4.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "2c08442b6150427fa9c29ccc078345a5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e7c48020d9a04c0b8a014b0cf6e77092",
       "placeholder": "​",
       "style": "IPY_MODEL_824b4befaef74aa3b12ade0b06d0b76c",
       "value": " 2/2 [02:26&lt;00:00, 69.29s/it]"
      }
     },
     "335456b476e142fda4e58898f4d36b0b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3ba37189f1f24ef2ae388999eab82bed": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "4e09eaaacf9a4d1aa458be421870ebc7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f227a02405bc473195d6f1e892f212d4",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_dd6f3470007b4c3c8557c014419f35bf",
       "value": 2.0
      }
     },
     "824b4befaef74aa3b12ade0b06d0b76c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "8306c23135c743c08cae77552b9cc355": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_335456b476e142fda4e58898f4d36b0b",
       "placeholder": "​",
       "style": "IPY_MODEL_3ba37189f1f24ef2ae388999eab82bed",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "dd6f3470007b4c3c8557c014419f35bf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "e7c48020d9a04c0b8a014b0cf6e77092": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f1afb102fb8b4154b5403b20da107aa8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f227a02405bc473195d6f1e892f212d4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fc6606039e0248769430e5073522610f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_8306c23135c743c08cae77552b9cc355",
        "IPY_MODEL_4e09eaaacf9a4d1aa458be421870ebc7",
        "IPY_MODEL_2c08442b6150427fa9c29ccc078345a5"
       ],
       "layout": "IPY_MODEL_f1afb102fb8b4154b5403b20da107aa8"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
