{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introducción\n\n\n## Objetivo\n\nUtilizar Llama 2.0, Langchain y ChromaDB para crear un sistema de Generación con Recuperación Mejorada (RAG). Esto nos permitirá hacer preguntas sobre nuestros documentos (que no se incluyeron en los datos de entrenamiento), sin necesidad de ajustar finamente el Modelo de Lenguaje Grande (LLM, por sus siglas en inglés).\nCuando se utiliza RAG, si se plantea una pregunta, primero se realiza un paso de recuperación para obtener documentos relevantes de una base de datos especial, una base de datos vectorial donde se indexaron estos documentos.\n\n## Definiciones\n\n* LLM - Modelo de Lenguaje Grande (Large Language Model)\n* Llama 2.0 - LLM de Meta\n* Langchain - un marco diseñado para simplificar la creación de aplicaciones utilizando LLM\n* Base de datos vectorial - una base de datos que organiza datos a través de vectores de alta dimensión\n* ChromaDB - base de datos vectorial\n* RAG - Generación con Recuperación Mejorada (consulte más detalles sobre RAG a continuación)\n\n## Detalles del modelo\n\n* **Modelo**: Llama 2\n* **Variante**: 7b-chat-hf (7b: 7 mil millones de dimensiones, hf: compilación de HuggingFace)\n* **Versión**: V1\n* **Framework**: PyTorch\n\nEl modelo LlaMA 2 está preentrenado y ajustado con 2 billones de tokens y de 7 a 70 mil millones de parámetros, lo que lo convierte en uno de los modelos de código abierto más potentes. Es una mejora significativa con respecto al modelo LlaMA 1.\n\n\n## ¿Qué es un sistema de Generación con Recuperación Mejorada (RAG)?\n\nLos Modelos de Lenguaje Grande (LLM) han demostrado su capacidad para comprender el contexto y proporcionar respuestas precisas a diversas tareas de Procesamiento de Lenguaje Natural (NLP), incluyendo la resumen, preguntas y respuestas, cuando se les solicita. Si bien son capaces de proporcionar respuestas muy buenas a preguntas sobre información con la que fueron entrenados, tienden a alucinar cuando el tema trata sobre información que \"no saben\", es decir, no estaba incluida en sus datos de entrenamiento. La Generación con Recuperación Mejorada combina recursos externos con LLM. Por lo tanto, los dos componentes principales de un sistema RAG son un recuperador y un generador.\n\nLa parte del recuperador se puede describir como un sistema que es capaz de codificar nuestros datos para que se puedan recuperar fácilmente las partes relevantes al consultarlos. La codificación se realiza utilizando incrustaciones de texto, es decir, un modelo entrenado para crear una representación vectorial de la información. La mejor opción para implementar un recuperador es una base de datos vectoriales. Como bases de datos vectoriales, existen múltiples opciones, tanto productos de código abierto como comerciales. Algunos ejemplos son ChromaDB, Mevius, FAISS, Pinecone, Weaviate. Nuestra opción en este cuaderno será una instancia local de ChromaDB (persistente).\n\nPara la parte del generador, la opción más obvia es un LLM. En este cuaderno utilizaremos un modelo LLaMA v2 cuantificado, de la colección de Modelos de Kaggle.\n\nLa orquestación del recuperador y el generador se realizará utilizando Langchain. Una función especializada de Langchain nos permite crear el recuperador-generador en una sola línea de código.\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Installations, imports, utils","metadata":{}},{"cell_type":"code","source":"!pip install transformers==4.33.0 accelerate==0.22.0 einops==0.6.1 langchain==0.0.300 xformers==0.0.21 \\\nbitsandbytes==0.41.1 sentence_transformers==2.2.2 chromadb==0.4.12","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-11-07T19:18:59.488766Z","iopub.execute_input":"2023-11-07T19:18:59.489485Z","iopub.status.idle":"2023-11-07T19:19:12.882823Z","shell.execute_reply.started":"2023-11-07T19:18:59.489449Z","shell.execute_reply":"2023-11-07T19:19:12.881584Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers==4.33.0 in /opt/conda/lib/python3.10/site-packages (4.33.0)\nRequirement already satisfied: accelerate==0.22.0 in /opt/conda/lib/python3.10/site-packages (0.22.0)\nRequirement already satisfied: einops==0.6.1 in /opt/conda/lib/python3.10/site-packages (0.6.1)\nRequirement already satisfied: langchain==0.0.300 in /opt/conda/lib/python3.10/site-packages (0.0.300)\nRequirement already satisfied: xformers==0.0.21 in /opt/conda/lib/python3.10/site-packages (0.0.21)\nRequirement already satisfied: bitsandbytes==0.41.1 in /opt/conda/lib/python3.10/site-packages (0.41.1)\nRequirement already satisfied: sentence_transformers==2.2.2 in /opt/conda/lib/python3.10/site-packages (2.2.2)\nRequirement already satisfied: chromadb==0.4.12 in /opt/conda/lib/python3.10/site-packages (0.4.12)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (0.16.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (0.3.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (4.66.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.22.0) (5.9.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.22.0) (2.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (2.0.17)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (3.8.4)\nRequirement already satisfied: anyio<4.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (3.7.0)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (4.0.2)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (0.6.0)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (1.33)\nRequirement already satisfied: langsmith<0.1.0,>=0.0.38 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (0.0.60)\nRequirement already satisfied: numexpr<3.0.0,>=2.8.4 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (2.8.5)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (1.10.9)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (8.2.2)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (0.15.1)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (1.11.2)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (3.2.4)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (0.1.99)\nRequirement already satisfied: chroma-hnswlib==0.7.3 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.12) (0.7.3)\nRequirement already satisfied: fastapi<0.100.0,>=0.95.2 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.12) (0.98.0)\nRequirement already satisfied: uvicorn[standard]>=0.18.3 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.12) (0.22.0)\nRequirement already satisfied: posthog>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.12) (3.0.2)\nRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.12) (4.6.3)\nRequirement already satisfied: pulsar-client>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.12) (3.3.0)\nRequirement already satisfied: onnxruntime>=1.14.1 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.12) (1.16.1)\nRequirement already satisfied: pypika>=0.48.9 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.12) (0.48.9)\nRequirement already satisfied: overrides>=7.3.1 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.12) (7.3.1)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.12) (5.12.0)\nRequirement already satisfied: bcrypt>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.12) (4.0.1)\nRequirement already satisfied: typer>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.12) (0.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (3.1.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (11.7.99)\nRequirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (11.7.99)\nRequirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (11.7.101)\nRequirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (8.5.0.96)\nRequirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (11.10.3.66)\nRequirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (10.9.0.58)\nRequirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (10.2.10.91)\nRequirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (11.4.0.1)\nRequirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (11.7.4.91)\nRequirement already satisfied: nvidia-nccl-cu11==2.14.3 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (2.14.3)\nRequirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (11.7.91)\nRequirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (2.0.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate==0.22.0) (68.0.0)\nRequirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate==0.22.0) (0.40.0)\nRequirement already satisfied: cmake in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate==0.22.0) (3.27.7)\nRequirement already satisfied: lit in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate==0.22.0) (17.0.4)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (1.3.1)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.300) (3.4)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.300) (1.3.0)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.300) (1.1.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.300) (3.20.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.300) (0.9.0)\nRequirement already satisfied: starlette<0.28.0,>=0.27.0 in /opt/conda/lib/python3.10/site-packages (from fastapi<0.100.0,>=0.95.2->chromadb==0.4.12) (0.27.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.0) (2023.9.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.0.300) (2.0)\nRequirement already satisfied: coloredlogs in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.12) (15.0.1)\nRequirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.12) (23.5.26)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.12) (3.20.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.33.0) (3.0.9)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.12) (1.16.0)\nRequirement already satisfied: monotonic>=1.5 in /opt/conda/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.12) (1.6)\nRequirement already satisfied: backoff>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.12) (2.2.1)\nRequirement already satisfied: python-dateutil>2.1 in /opt/conda/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.12) (2.8.2)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from pulsar-client>=3.1.0->chromadb==0.4.12) (2023.7.22)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.33.0) (1.26.15)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.300) (2.0.2)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer>=0.9.0->chromadb==0.4.12) (8.1.7)\nRequirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (0.14.0)\nRequirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (0.6.0)\nRequirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (1.0.0)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (0.17.0)\nRequirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (0.20.0)\nRequirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (11.0.3)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers==2.2.2) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers==2.2.2) (3.1.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->sentence_transformers==2.2.2) (9.5.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.300) (1.0.0)\nRequirement already satisfied: humanfriendly>=9.1 in /opt/conda/lib/python3.10/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.4.12) (10.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.22.0) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.22.0) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install docx2txt","metadata":{"execution":{"iopub.status.busy":"2023-11-07T19:19:12.884963Z","iopub.execute_input":"2023-11-07T19:19:12.885304Z","iopub.status.idle":"2023-11-07T19:19:25.060874Z","shell.execute_reply.started":"2023-11-07T19:19:12.885278Z","shell.execute_reply":"2023-11-07T19:19:25.059659Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: docx2txt in /opt/conda/lib/python3.10/site-packages (0.8)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch import cuda, bfloat16\nimport torch\nimport transformers\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom time import time\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain import document_loaders \nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.chains import RetrievalQA\nfrom langchain.vectorstores import Chroma\nfrom langchain.prompts import PromptTemplate\nfrom bs4 import BeautifulSoup as Soup","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-07T19:19:25.062667Z","iopub.execute_input":"2023-11-07T19:19:25.063080Z","iopub.status.idle":"2023-11-07T19:19:30.407996Z","shell.execute_reply.started":"2023-11-07T19:19:25.063030Z","shell.execute_reply":"2023-11-07T19:19:30.406917Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Inicializar modelo, tokenizador, y canal de consultas.","metadata":{}},{"cell_type":"markdown","source":"Define el modelo, el dispositivo y la configuración de `bitsandbytes`.","metadata":{}},{"cell_type":"code","source":"model_id = '/kaggle/input/llama-2/pytorch/7b-chat-hf/1'\n\ndevice = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n\n# set quantization configuration to load large model with less GPU memory\n# this requires the `bitsandbytes` library\nbnb_config = transformers.BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type='nf4',\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=bfloat16\n)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-07T19:19:30.410394Z","iopub.execute_input":"2023-11-07T19:19:30.410982Z","iopub.status.idle":"2023-11-07T19:19:30.498133Z","shell.execute_reply.started":"2023-11-07T19:19:30.410946Z","shell.execute_reply":"2023-11-07T19:19:30.497308Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Preparar el modelo y el tokenizador.","metadata":{}},{"cell_type":"code","source":"time_1 = time()\n\nmodel_config = transformers.AutoConfig.from_pretrained(\n    model_id,\n)\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n    model_id,\n    trust_remote_code=True,\n    config=model_config,\n    quantization_config=bnb_config,\n    device_map='auto',\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\ntime_2 = time()\nprint(f\"Prepare model, tokenizer: {round(time_2-time_1, 3)} sec.\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-07T19:19:30.499318Z","iopub.execute_input":"2023-11-07T19:19:30.499618Z","iopub.status.idle":"2023-11-07T19:22:49.624233Z","shell.execute_reply.started":"2023-11-07T19:19:30.499593Z","shell.execute_reply":"2023-11-07T19:22:49.623222Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf6453a43a9d40ae854bde4e81f79a8b"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Prepare model, tokenizer: 199.118 sec.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Definir el query pipeline","metadata":{}},{"cell_type":"code","source":"time_1 = time()\n\nquery_pipeline = transformers.pipeline(\n        \"text-generation\",\n        model=model,\n        tokenizer=tokenizer,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",)\n\ntime_2 = time()\nprint(f\"Prepare pipeline: {round(time_2-time_1, 3)} sec.\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-07T19:22:49.626558Z","iopub.execute_input":"2023-11-07T19:22:49.626951Z","iopub.status.idle":"2023-11-07T19:22:51.624287Z","shell.execute_reply.started":"2023-11-07T19:22:49.626915Z","shell.execute_reply":"2023-11-07T19:22:51.623333Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Prepare pipeline: 1.992 sec.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Definir una función para testear el pipeline","metadata":{}},{"cell_type":"code","source":"def test_model(tokenizer, pipeline, prompt_to_test):\n    \"\"\"\n    Perform a query\n    print the result\n    Args:\n        tokenizer: the tokenizer\n        pipeline: the pipeline\n        prompt_to_test: the prompt\n    Returns\n        None\n    \"\"\"\n    # adapted from https://huggingface.co/blog/llama2#using-transformers\n    time_1 = time()\n    sequences = pipeline(\n        prompt_to_test,\n        do_sample=True,\n        top_k=10,\n        num_return_sequences=1,\n        eos_token_id=tokenizer.eos_token_id,\n        max_length=200,)\n    time_2 = time()\n    print(f\"Test inference: {round(time_2-time_1, 3)} sec.\")\n    for seq in sequences:\n        print(f\"Result: {seq['generated_text']}\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-07T19:22:51.625444Z","iopub.execute_input":"2023-11-07T19:22:51.625723Z","iopub.status.idle":"2023-11-07T19:22:51.632049Z","shell.execute_reply.started":"2023-11-07T19:22:51.625699Z","shell.execute_reply":"2023-11-07T19:22:51.631115Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Testear la query pipeline\n\nTesteamos el pipeline con una query sobre...","metadata":{}},{"cell_type":"code","source":"test_model(tokenizer,\n           query_pipeline,\n           \"Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words.\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-07T19:23:05.592034Z","iopub.execute_input":"2023-11-07T19:23:05.592444Z","iopub.status.idle":"2023-11-07T19:23:17.386837Z","shell.execute_reply.started":"2023-11-07T19:23:05.592416Z","shell.execute_reply":"2023-11-07T19:23:17.385757Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1417: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Test inference: 11.79 sec.\nResult: Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words.\nThe State of the Union address is an annual speech delivered by the President of the United States to Congress, in which the President reviews the current state of the nation and outlines his or her legislative agenda for the upcoming year.\nState of the Union address is a speech given by the President to Congress, it's an annual event where the President reviews the current state of the nation and presents his legislative agenda for the upcoming year\n","output_type":"stream"}]},{"cell_type":"code","source":"test_model(tokenizer,\n           query_pipeline,\n           \"Explica que es el discurso sobre el estado de la nación en EE.UU. Hazlo en menos de 100 palabras.\")","metadata":{"execution":{"iopub.status.busy":"2023-11-07T19:23:17.389827Z","iopub.execute_input":"2023-11-07T19:23:17.390448Z","iopub.status.idle":"2023-11-07T19:23:30.708647Z","shell.execute_reply.started":"2023-11-07T19:23:17.390419Z","shell.execute_reply":"2023-11-07T19:23:30.707564Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Test inference: 13.314 sec.\nResult: Explica que es el discurso sobre el estado de la nación en EE.UU. Hazlo en menos de 100 palabras.\n\nEl discurso sobre el estado de la nación en EE.UU. es un mensaje anual que el presidente de los Estados Unidos lee a Congress, en el que describe el estado actual de la nación y los desafíos que enfrenta.\n\nEste discurso es una oportunidad para el líder de la nación evaluar el progreso realizado en diferentes áreas, como la económica, la social y la internacional, y para presentar propuestas para mejorar el estado de la nación. Además, el discurso es un momento crítico para establecer la agenda política del gobierno durante el próximo año.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Retrieval Augmented Generation","metadata":{}},{"cell_type":"markdown","source":"## Comprobar el modelo con HuggingFace pipeline\n\n\nTesteamos el modelo con HF pipeline, usando una query sobre.","metadata":{"execution":{"iopub.status.busy":"2023-09-23T19:22:16.433666Z","iopub.execute_input":"2023-09-23T19:22:16.434937Z","iopub.status.idle":"2023-09-23T19:22:16.440864Z","shell.execute_reply.started":"2023-09-23T19:22:16.434891Z","shell.execute_reply":"2023-09-23T19:22:16.439217Z"}}},{"cell_type":"code","source":"llm = HuggingFacePipeline(pipeline=query_pipeline)\n# checking again that everything is working fine\nllm(prompt=\"Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words.\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-07T19:23:39.052153Z","iopub.execute_input":"2023-11-07T19:23:39.052540Z","iopub.status.idle":"2023-11-07T19:23:44.390534Z","shell.execute_reply.started":"2023-11-07T19:23:39.052510Z","shell.execute_reply":"2023-11-07T19:23:44.389572Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"'\\nThe State of the Union address is an annual speech given by the President of the United States to a joint session of Congress, in which the President reports on the current state of the union and outlines their legislative agenda for the upcoming year.'"},"metadata":{}}]},{"cell_type":"code","source":"llm = HuggingFacePipeline(pipeline=query_pipeline)\n# checking again that everything is working fine\nllm(prompt=\"Explica que es el discurso sobre el estado de la nación en EE.UU. Hazlo en menos de 100 palabras.\")","metadata":{"execution":{"iopub.status.busy":"2023-11-07T19:23:44.392330Z","iopub.execute_input":"2023-11-07T19:23:44.392658Z","iopub.status.idle":"2023-11-07T19:23:55.840326Z","shell.execute_reply.started":"2023-11-07T19:23:44.392634Z","shell.execute_reply":"2023-11-07T19:23:55.839432Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"'\\n\\nEl discurso sobre el estado de la nación en EE.UU. es un mensaje anual que el presidente de los Estados Unidos lee ante el Congreso y el público en general. El discurso se centra en la situación actual de la nación, incluyendo el estado de la economía, la seguridad nacional, la educación y la política exterior. El presidente utiliza este momento para informar a los ciudadanos sobre los logros y desafíos de la nación y para establecer objetivos y metas para el futuro.'"},"metadata":{}}]},{"cell_type":"markdown","source":"## Data Ingestion usando Loaders\n\nVamos a adquirir información de distintas fuentes y a juntarla.","metadata":{}},{"cell_type":"code","source":"# Adquirir datos de distintas fuentes\nloaders = [\n    #document_loaders.Docx2txtLoader(\"/kaggle/input/tfg-datasetstest/Listado Preguntas-Respuestas - ONLINE.docx\"),\n    #document_loaders.Docx2txtLoader(\"/kaggle/input/tfg-datasetstest/Listado Preguntas-Respuestas - PRESENCIAL.docx\"),\n    document_loaders.TextLoader(\"/kaggle/input/tfg-datasetstest/Listado Preguntas-Respuestas - ONLINE.txt\", encoding='utf8'),\n    #document_loaders.PyPDFLoader(\"/kaggle/input/tfg-datasetstest/2-entrega_proyecto_fin_de_grado_vena.pdf\"),\n    #document_loaders.PyPDFLoader(\"/kaggle/input/tfg-datasetstest/3-vena-modelo_informe_tutor_academico_modificado.pdf\"),\n    document_loaders.PyPDFLoader(\"/kaggle/input/tfg-datasetstest/reglamentp_tfg-tfm_aprob._08-06-2022.pdf\"),\n    #document_loaders.GitHubIssuesLoader(repo=\"langchain-ai/langchain\",access_token=ACCESS_TOKEN, creator=\"UmerHA\",)\n    \n    #document_loaders.recursive_url_loader.RecursiveUrlLoader(url=\"https://clopezno.github.io/tfg_gii_online/HistoricoSist.html\", \n    #                                                         max_depth=2, extractor=lambda x: Soup(x, \"html.parser\").text),\n    \n    #document_loaders.WebBaseLoader(\"https://www.ubu.es/grado-en-ingenieria-informatica/informacion-basica/trabajo-fin-de-grado\")   \n]\n\ndocuments = []\n\nfor loader in loaders:\n    documents.extend(loader.load())","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-07T19:45:16.868166Z","iopub.execute_input":"2023-11-07T19:45:16.868826Z","iopub.status.idle":"2023-11-07T19:45:17.616884Z","shell.execute_reply.started":"2023-11-07T19:45:16.868789Z","shell.execute_reply":"2023-11-07T19:45:17.616104Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"## Trocear los datos en chunks\n\nDividimos los datos en chunks utilizando un separador de texto de caracteres recursivo.","metadata":{}},{"cell_type":"code","source":"text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\nall_splits = text_splitter.split_documents(documents)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-07T19:45:17.618166Z","iopub.execute_input":"2023-11-07T19:45:17.618535Z","iopub.status.idle":"2023-11-07T19:45:17.627111Z","shell.execute_reply.started":"2023-11-07T19:45:17.618501Z","shell.execute_reply":"2023-11-07T19:45:17.626101Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":"## Crear Embeddings y guardarlos en una BD Vectorial\n\nCreate the embeddings using Sentence Transformer and HuggingFace embeddings.","metadata":{}},{"cell_type":"code","source":"model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\nmodel_kwargs = {\"device\": \"cuda\"}\n\nembeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-07T19:45:17.628277Z","iopub.execute_input":"2023-11-07T19:45:17.628617Z","iopub.status.idle":"2023-11-07T19:45:17.970965Z","shell.execute_reply.started":"2023-11-07T19:45:17.628585Z","shell.execute_reply":"2023-11-07T19:45:17.970114Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":"Inicializar ChromaDB con los chunks, embeddings y con persistencia local.","metadata":{}},{"cell_type":"code","source":"persist_directory=\"chroma_db\"\n!rm -rf ./chroma_db  # remove old database files if any\nvectordb = Chroma.from_documents(\n    documents=all_splits, \n    embedding=embeddings, \n    persist_directory=persist_directory)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-07T19:45:17.972599Z","iopub.execute_input":"2023-11-07T19:45:17.972903Z","iopub.status.idle":"2023-11-07T19:45:19.618528Z","shell.execute_reply.started":"2023-11-07T19:45:17.972876Z","shell.execute_reply":"2023-11-07T19:45:19.617648Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3b272e4c60b46e88a9d471edb4aa7f1"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Custom Prompt\n\nCrear un Propmpt customizado para forzar cotexto e idioma.","metadata":{}},{"cell_type":"code","source":"prompt_template = \"\"\"<s>[INST] You are given the context after <<CONTEXT>> and a question after <<QUESTION>>.\n\nYou are a chatbot for the preparation of Thesis project(TFG). Answer in the same language {question}.\n\n<<QUESTION>>{question}\\n<<CONTEXT>>{context} [/INST]\"\"\"\nPROMPT = PromptTemplate(\n    template=prompt_template, \n    input_variables=[\"context\", \"question\"]\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T19:55:02.522023Z","iopub.execute_input":"2023-11-07T19:55:02.522933Z","iopub.status.idle":"2023-11-07T19:55:02.527757Z","shell.execute_reply.started":"2023-11-07T19:55:02.522897Z","shell.execute_reply":"2023-11-07T19:55:02.526788Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"markdown","source":"## Inicializar chain","metadata":{}},{"cell_type":"code","source":"# Retrieve more documents with higher diversity- useful if your dataset has many similar documents\n#retriever = vectordb.as_retriever(search_kwargs = {\"k\": 5, \"search_type\" : \"similarity\"})\nretriever = vectordb.as_retriever()\n\nqa = RetrievalQA.from_chain_type(\n    llm=llm, \n    chain_type=\"stuff\", \n    retriever=retriever, \n    chain_type_kwargs={\"prompt\": PROMPT},\n    verbose=False\n)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-07T19:55:50.049751Z","iopub.execute_input":"2023-11-07T19:55:50.050196Z","iopub.status.idle":"2023-11-07T19:55:50.056704Z","shell.execute_reply.started":"2023-11-07T19:55:50.050160Z","shell.execute_reply":"2023-11-07T19:55:50.055732Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"markdown","source":"## Test Retrieval-Augmented Generation \n\n\nDefinimos una función de prueba que ejecutará la consulta y medirá el tiempo.","metadata":{}},{"cell_type":"code","source":"def test_rag(qa, query):\n    print(f\"Query: {query}\\n\")\n    time_1 = time()\n    result = qa.run(query)\n    time_2 = time()\n    print(f\"Inference time: {round(time_2-time_1, 3)} sec.\")\n    print(\"\\nResult: \", result)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-07T19:55:54.559975Z","iopub.execute_input":"2023-11-07T19:55:54.560392Z","iopub.status.idle":"2023-11-07T19:55:54.566014Z","shell.execute_reply.started":"2023-11-07T19:55:54.560362Z","shell.execute_reply":"2023-11-07T19:55:54.564833Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"markdown","source":"Comprobemos algunas consultas.","metadata":{}},{"cell_type":"code","source":"query = \"¿Se pueden adjuntar videos en el depósito del TFG??\"\ntest_rag(qa, query)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-07T19:55:59.725080Z","iopub.execute_input":"2023-11-07T19:55:59.725480Z","iopub.status.idle":"2023-11-07T19:56:30.078854Z","shell.execute_reply.started":"2023-11-07T19:55:59.725449Z","shell.execute_reply":"2023-11-07T19:56:30.077784Z"},"trusted":true},"execution_count":86,"outputs":[{"name":"stdout","text":"Query: ¿Se pueden adjuntar videos en el depósito del TFG??\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"162995c9c3984720a05d9260aa25c7cf"}},"metadata":{}},{"name":"stdout","text":"Inference time: 30.348 sec.\n\nResult:    ¡Hola! ¡Claro que sí! Puedes adjuntar videos en el depósito del TFG. En la convocatoria de la asignatura, se habilita una tarea en UBUVirtual donde puedes subir los ficheros necesarios, incluyendo videos, para la evaluación del TFG.\n\nEs importante mencionar que los videos debe estar colgados en YouTube y proporcionar los enlaces en el depósito del TFG. Esto se debe a que la plataforma de UBUVirtual no permite la subida de ficheros directamente desde el ordenador del estudiante.\n\nRecuerda que la presentación y defensa del TFG pueden ser realizadas en modalidad on-line, por lo que es importante estar preparado para presentar y defender tu trabajo de manera efectiva a través de la videoconferencia.\n\nSi tienes alguna otra pregunta o necesitas más información, no dudes en preguntar. Estoy aquí para ayudarte en lo que pueda. ¡Buena suerte con tu TFG!\n","output_type":"stream"}]},{"cell_type":"code","source":"query = \"¿Puedo cambiar de tutor o tema?\"\ntest_rag(qa, query)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-07T19:56:30.080709Z","iopub.execute_input":"2023-11-07T19:56:30.081035Z","iopub.status.idle":"2023-11-07T19:57:20.110373Z","shell.execute_reply.started":"2023-11-07T19:56:30.081008Z","shell.execute_reply":"2023-11-07T19:57:20.109387Z"},"trusted":true},"execution_count":87,"outputs":[{"name":"stdout","text":"Query: ¿Puedo cambiar de tutor o tema?\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ae834f605404c99bf46463f779d37a3"}},"metadata":{}},{"name":"stdout","text":"Inference time: 50.025 sec.\n\nResult:    ¡Hola! ¡Claro que sí! Puedes cambiar de tutor o tema en tu TFG. Para hacerlo, deberás solicitarlo al Tribunal por escrito, en un plazo máximo de un mes desde la fecha de la publicación de la asignación. El Tribunal estará obligado a contestar al alumno por escrito en un plazo máximo de siete días hábiles, en el que se motivará la resolución del TFG/TFM a la petición del alumno.\n\nEn cuanto a la plantilla para la memoria, existente un modelo de plantilla en LaTeX en el siguiente enlace: <https://github.com/ubutfgm/plantillaLatex>.\n\nTambién puedes elegir tutor dependiendo de la modalidad de proyecto. En la modalidad C, el estudiante tiene que acordar previamente su propuesta con un tutor que acepte su temática. En la modalidad A, las propuestas de TFG concretas se publican y los tutores pueden indicar su interés en trabajar con ciertas propuestas. En la modalidad B, las empresas ofrecen propuestas de TFG y el alumno puede elegir el tutor que desee trabajar.\n\nEn cuanto a la copia impresa de la memoria, si vives fuera de Burgos, en la modalidad online no se entrega copia impresa. En cambio, deberás enviar la memoria y los anexos a la secretaria a través de la plataforma de UBUVirtual.\n\nEn cuanto a un ejemplo de TFG, una buena opción es identificar los conceptos teóricos principales que se necesitan en el TFG y buscar a los profesores de las guías docentes de las asignaturas del Grado que te los enseñaron.\n\nFinalmente, si tienes alguna pregunta o necesitas un consejo, no dudes en preguntar. Estoy aquí para ayudarte en lo que pueda. ¡Buena suerte con tu TFG!\n","output_type":"stream"}]},{"cell_type":"code","source":"query = \"¿Que pasaria si no me gusta el tema o tutor que he escogido?\"\ntest_rag(qa, query)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T19:57:20.111868Z","iopub.execute_input":"2023-11-07T19:57:20.112789Z","iopub.status.idle":"2023-11-07T19:58:13.710323Z","shell.execute_reply.started":"2023-11-07T19:57:20.112751Z","shell.execute_reply":"2023-11-07T19:58:13.709286Z"},"trusted":true},"execution_count":88,"outputs":[{"name":"stdout","text":"Query: ¿Que pasaria si no me gusta el tema o tutor que he escogido?\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a14b335a30348cdaea58e2bdca941f2"}},"metadata":{}},{"name":"stdout","text":"Inference time: 53.593 sec.\n\nResult:    ¡Hola! Si no te gusta el tema o el tutor que has escogido para tu TFG, no te preocupes, es normal sentir dudas o deseos de cambiar en algún momento del proceso.\n\nEn cuanto a la elección del tutor, en la modalidad C de oferta de TFG por el alumno, es recomendable identificar los conceptos teóricos principales que se necesitan en el TFG y buscar a los profesores de las guías docentes de las asignaturas del Grado que te los enseñaron. Esto te ayudará a encontrar a un tutor que tenga experiencia y conocimientos en el área de estudio que te interesa.\n\nSi no puedes elegir a tu tutor, en la modalidad A, puedes presentar una propuesta de TFG concreta y publicarla en el portal de UBUVirtual. En la modalidad B, las empresas ofrecen la oportunidad de trabajar con un tutor de la asignatura que acepten la propuesta de TFG.\n\nSi deseas cambiar de tutor o tema, puedes solicitarlo al Tribunal por escrito, en un plazo máximo de un mes desde la fecha de la publicación de la asignación. El Tribunal estará obligado a contestar al alumno por escrito en un plazo máximo de siete días hábiles, y en dicho escrito se motivará la resolución del TFG/TFM a la petición del alumno.\n\nEn cuanto a la dificultad de aprobar el TFG, es importante tener en cuenta que es un proyecto de gran complejidad que requiere tiempo y esfuerzo para desarrollar. Sin embargo, si estás dispuesto a trabajar duro y a seguir las instrucciones del tutor y del Tribunal, es posible aprobar el TFG.\n\nEn resumen, no te desanimes si no te gusta el tema o el tutor que has escogido. Puedes cambiar de tutor o tema y seguir trabajando duro para aprobar el TFG. Si tienes alguna duda o problema, no dudes en preguntar a tu tutor o en contactar al Tribunal. ¡Buena suerte en tu TFG!\n","output_type":"stream"}]},{"cell_type":"code","source":"query = \"¿Can you give examples of past TFG?\"\ntest_rag(qa, query)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T19:58:13.712975Z","iopub.execute_input":"2023-11-07T19:58:13.713479Z","iopub.status.idle":"2023-11-07T19:58:57.903488Z","shell.execute_reply.started":"2023-11-07T19:58:13.713440Z","shell.execute_reply":"2023-11-07T19:58:57.902324Z"},"trusted":true},"execution_count":89,"outputs":[{"name":"stdout","text":"Query: ¿Can you give examples of past TFG?\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bcb5ccf08484f8b9230273e436adace"}},"metadata":{}},{"name":"stdout","text":"Inference time: 44.185 sec.\n\nResult:    Examples of past TFG (Thesis Project) include:\n\n1. Development of a Machine Learning Model for Predicting Customer Churn in a Telecommunications Company: This TFG aimed to create a machine learning model that could predict customer churn in a telecommunications company, using data from various sources such as customer demographics, usage patterns, and feedback.\n2. Design and Implementation of a Smart Home Automation System using IoT and Cloud Computing: This TFG focused on the design and implementation of a smart home automation system using Internet of Things (IoT) devices and cloud computing. The system was designed to provide a comfortable and efficient living space for the elderly and people with disabilities.\n3. Development of a Mobile Application for Monitoring and Managing Chronic Diseases: This TFG developed a mobile application that allowed users to monitor and manage chronic diseases such as diabetes, hypertension, and asthma. The application provided a platform for patients to track their health data, receive alerts and reminders, and communicate with healthcare professionals.\n4. Analysis of the Impact of Social Media on Political Discourse: This TFG examined the impact of social media on political discourse, focusing on the role of social media in shaping public opinion and influencing political decisions. The study used a mixed-methods approach, combining quantitative and qualitative data analysis.\n5. Design and Implementation of a Sustainable Supply Chain Management System for a Fashion Company: This TFG aimed to design and implement a sustainable supply chain management system for a fashion company. The system was designed to reduce waste, minimize environmental impact, and improve supply chain efficiency.\n\nThese are just a few examples of past TFG projects. The specific topics and objectives of TFG projects can vary widely depending on the field of study, the interests of the student, and the needs of the company or organization involved in the project.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Fuentes del documento\n\nVerifiquemos las fuentes de documentos para la última consulta ejecutada.","metadata":{}},{"cell_type":"code","source":"docs = vectordb.similarity_search(query)\nprint(f\"Query: {query}\")\nprint(f\"Retrieved documents: {len(docs)}\")\nfor doc in docs:\n    doc_details = doc.to_json()['kwargs']\n    print(\"Source: \", doc_details['metadata']['source'])\n    print(\"Text: \", doc_details['page_content'], \"\\n\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-07T19:45:16.822900Z","iopub.execute_input":"2023-11-07T19:45:16.823230Z","iopub.status.idle":"2023-11-07T19:45:16.864912Z","shell.execute_reply.started":"2023-11-07T19:45:16.823191Z","shell.execute_reply":"2023-11-07T19:45:16.864076Z"},"trusted":true},"execution_count":57,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89efc34fd67e427880ed6bdb3528a4b8"}},"metadata":{}},{"name":"stdout","text":"Query: ¿Can you give examples of past TFG?\nRetrieved documents: 4\nSource:  /kaggle/input/tfg-datasetstest/Listado Preguntas-Respuestas - ONLINE.docx\nText:  que_es_tfg\n\n¿Qué es el TFG?\n\nUn TFG es aplicación autónoma e integral de algunos conceptos aprendidos en las distintas asignaturas del Grado. El estudiante actúa como gestor del proceso de para resolver los problemas planteados en TFG. Todo TFG tiene un tutor o tutora del Grado asignado.\n\nque_es_un_anexo\n\n¿Qué es un anexo?\n\nUn anexo es un documento explicativo de las características técnicas del TFG a definir por consenso con el tutor/a. Se recomienda seguir la estructura disponible en el siguiente enlace:\n\nhttps://github.com/ubutfgm/plantillaLatex\n\nquienes_estaran_ exposicion\n\n¿Quiénes estarán en la exposición?\n\nLa presentación/exposición de los TFG es un acto social y público donde exponer los trabajos a los asistentes que se lo soliciten al tribunal. \n\nSource:  /kaggle/input/tfg-datasetstest/Listado Preguntas-Respuestas - ONLINE.docx\nText:  Dame un consejo\n\nRealiza una elección de TFG relacionado con asignaturas que te hayan gustado del Grado para estar motivado. Autofórmate y busca bibliografía específicas para resolver los problemas del TFG. Consulta TFG de cursos anteriores, tanto desarrollos en Github, como la memoria descriptiva. Planifícate bien el tiempo de tu TFG. El tiempo para realizar un TFG depende de muchos factores y ahora tú gestionas tu propio proceso de aprendizaje.\n\nConvalidar\n\n¿Se puede convalidar el TFG?\n\n No, el TFG no se puede reconocer.\n\nconversacion_ofensiva\n\n*CUALQUIER INSULTO*\n\nSiento si no te he sido de utilidad. Dame otra oportunidad, estoy aquí para aprender y ayudarte.\n\nconvocatorias_curso\n\n¿Cuántas convocatorias tengo por curso?\n\nCada curso el estudiante que se matricula del TFG tiene derecho a dos convocatorias de evaluación.\n\nCovid\n\n¿Hay algo a tener en cuenta a raíz del Covid? \n\nSource:  /kaggle/input/tfg-datasetstest/Listado Preguntas-Respuestas - ONLINE.docx\nText:  No, no se puede cambiar la fecha de la presentación. \n\nPorcentaje nota \n\n¿Qué porcentaje de la nota es la memoria y cuál el trabajo y presentación?\n\nConsulta la guía docente de la asignatura.\n\nRúbrica\n\nRúbrica\n\nNo existe una rúbrica como tal, dada la diversidad de TFGs, sino un procedimiento de evaluación general indicado en la guía docente de la asignatura.\n\nComunidad_tfgs\n\n¿Dónde está la comunidad de tfgs?\n\nLa comunidad de TFG se encuentra en el listado de asignaturas disponibles en UBUVirtual (solo visible para el coordinador del grado y los alumnos matriculados en la asignatura)\n\nPortada_CD\n\n¿Dónde puedo encontrar la portada de los CDs??\n\n\n\nExiste un fichero disponible en la asignatura de TFG con nombre caratulaCD. \n\nSource:  /kaggle/input/tfg-datasetstest/Listado Preguntas-Respuestas - ONLINE.docx\nText:  No se responden cuestiones generales que no sean relativas al TFG.\n\nplantilla_memoria\n\n¿Existe alguna plantilla para la memoria?\n\nExiste un modelo de plantilla en LaTeX a seguir lo más fielmente posible en el siguiente enlace:\n\nhttps://github.com/ubutfgm/plantillaLatex\n\npuedo_elegir_tutor\n\n¿Puedo elegir tutor?\n\nPuedes elegir tutor dependiendo de la modalidad de proyecto.\n\nEn la modalidad C el estudiante tiene que acordar previamente su propuesta con un tutor que acepte su temática.\n\nModalidad A: propuestas de TFG concretas que publiquen los tutores\n\nModalidad B: propuestas de TFG que ofertan empresas previo acuerdo con un tutor de la asignatura \n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Conclusión\n\nUtilizamos Langchain, ChromaDB y Llama 2 como un Large Language Model (LLM) para construir una solución con Retrieval-Augmented Generation. Para las pruebas, estábamos utilizando...\n\n\n# Mas trabajos relacionados \n\n* https://www.kaggle.com/code/gpreda/test-llama-2-quantized-with-llama-cpp (quantizing LLama 2 model using llama.cpp)\n* https://www.kaggle.com/code/gpreda/fast-test-of-llama-v2-pre-quantized-with-llama-cpp  (quantized Llamam 2 model using llama.cpp)  \n* https://www.kaggle.com/code/gpreda/test-of-llama-2-quantized-with-llama-cpp-on-cpu (quantized model using llama.cpp - running on CPU)\n+ https://www.kaggle.com/code/acorn8/q-a-with-llms-harry-potter-mistral-hf-api (CFG Class)\n","metadata":{}},{"cell_type":"markdown","source":"# References  \n\n[1] Murtuza Kazmi, Using LLaMA 2.0, FAISS and LangChain for Question-Answering on Your Own Data, https://medium.com/@murtuza753/using-llama-2-0-faiss-and-langchain-for-question-answering-on-your-own-data-682241488476  \n\n[2] Patrick Lewis, Ethan Perez, et. al., Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, https://browse.arxiv.org/pdf/2005.11401.pdf \n\n[3] Minhajul Hoque, Retrieval Augmented Generation: Grounding AI Responses in Factual Data, https://medium.com/@minh.hoque/retrieval-augmented-generation-grounding-ai-responses-in-factual-data-b7855c059322  \n\n[4] Fangrui Liu\t, Discover the Performance Gain with Retrieval Augmented Generation, https://thenewstack.io/discover-the-performance-gain-with-retrieval-augmented-generation/\n\n[5] Andrew, How to use Retrieval-Augmented Generation (RAG) with Llama 2, https://agi-sphere.com/retrieval-augmented-generation-llama2/   \n\n[6] Yogendra Sisodia, Retrieval Augmented Generation Using Llama2 And Falcon, https://medium.com/@scholarly360/retrieval-augmented-generation-using-llama2-and-falcon-ed26c7b14670\n\n[7] Using a Retriever in Langchain - https://python.langchain.com/docs/use_cases/question_answering/vector_db_qa","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}