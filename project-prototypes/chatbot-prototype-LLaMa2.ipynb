{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introducción\n\n\n## Objetivo\n\nUtilizar Llama 2.0, Langchain y ChromaDB para crear un sistema de Generación con Recuperación Mejorada (RAG). Esto nos permitirá hacer preguntas sobre nuestros documentos (que no se incluyeron en los datos de entrenamiento), sin necesidad de ajustar finamente el Modelo de Lenguaje Grande (LLM, por sus siglas en inglés).\nCuando se utiliza RAG, si se plantea una pregunta, primero se realiza un paso de recuperación para obtener documentos relevantes de una base de datos especial, una base de datos vectorial donde se indexaron estos documentos.\n\n## Definiciones\n\n* LLM - Modelo de Lenguaje Grande (Large Language Model)\n* Llama 2.0 - LLM de Meta\n* Langchain - un marco diseñado para simplificar la creación de aplicaciones utilizando LLM\n* Base de datos vectorial - una base de datos que organiza datos a través de vectores de alta dimensión\n* ChromaDB - base de datos vectorial\n* RAG - Generación con Recuperación Mejorada (consulte más detalles sobre RAG a continuación)\n\n## Detalles del modelo\n\n* **Modelo**: Llama 2\n* **Variante**: 7b-chat-hf (7b: 7 mil millones de dimensiones, hf: compilación de HuggingFace)\n* **Versión**: V1\n* **Framework**: PyTorch\n\nEl modelo LlaMA 2 está preentrenado y ajustado con 2 billones de tokens y de 7 a 70 mil millones de parámetros, lo que lo convierte en uno de los modelos de código abierto más potentes. Es una mejora significativa con respecto al modelo LlaMA 1.\n\n\n## ¿Qué es un sistema de Generación con Recuperación Mejorada (RAG)?\n\nLos Modelos de Lenguaje Grande (LLM) han demostrado su capacidad para comprender el contexto y proporcionar respuestas precisas a diversas tareas de Procesamiento de Lenguaje Natural (NLP), incluyendo la resumen, preguntas y respuestas, cuando se les solicita. Si bien son capaces de proporcionar respuestas muy buenas a preguntas sobre información con la que fueron entrenados, tienden a alucinar cuando el tema trata sobre información que \"no saben\", es decir, no estaba incluida en sus datos de entrenamiento. La Generación con Recuperación Mejorada combina recursos externos con LLM. Por lo tanto, los dos componentes principales de un sistema RAG son un recuperador y un generador.\n\nLa parte del recuperador se puede describir como un sistema que es capaz de codificar nuestros datos para que se puedan recuperar fácilmente las partes relevantes al consultarlos. La codificación se realiza utilizando incrustaciones de texto, es decir, un modelo entrenado para crear una representación vectorial de la información. La mejor opción para implementar un recuperador es una base de datos vectoriales. Como bases de datos vectoriales, existen múltiples opciones, tanto productos de código abierto como comerciales. Algunos ejemplos son ChromaDB, Mevius, FAISS, Pinecone, Weaviate. Nuestra opción en este cuaderno será una instancia local de ChromaDB (persistente).\n\nPara la parte del generador, la opción más obvia es un LLM. En este cuaderno utilizaremos un modelo LLaMA v2 cuantificado, de la colección de Modelos de Kaggle.\n\nLa orquestación del recuperador y el generador se realizará utilizando Langchain. Una función especializada de Langchain nos permite crear el recuperador-generador en una sola línea de código.\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Installations, imports, utils","metadata":{}},{"cell_type":"code","source":"!pip install transformers==4.33.0 accelerate==0.22.0 einops==0.6.1 langchain==0.0.300 xformers==0.0.21 \\\nbitsandbytes==0.41.1 sentence_transformers==2.2.2 chromadb==0.4.12","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-11-07T12:13:10.186179Z","iopub.execute_input":"2023-11-07T12:13:10.187034Z","iopub.status.idle":"2023-11-07T12:16:21.356177Z","shell.execute_reply.started":"2023-11-07T12:13:10.186999Z","shell.execute_reply":"2023-11-07T12:16:21.355081Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers==4.33.0 in /opt/conda/lib/python3.10/site-packages (4.33.0)\nRequirement already satisfied: accelerate==0.22.0 in /opt/conda/lib/python3.10/site-packages (0.22.0)\nCollecting einops==0.6.1\n  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting langchain==0.0.300\n  Downloading langchain-0.0.300-py3-none-any.whl (1.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting xformers==0.0.21\n  Downloading xformers-0.0.21-cp310-cp310-manylinux2014_x86_64.whl (167.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.0/167.0 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting bitsandbytes==0.41.1\n  Downloading bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting sentence_transformers==2.2.2\n  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting chromadb==0.4.12\n  Downloading chromadb-0.4.12-py3-none-any.whl (426 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.5/426.5 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (0.16.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (0.3.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (4.66.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.22.0) (5.9.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.22.0) (2.0.0)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (2.0.17)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (3.8.4)\nRequirement already satisfied: anyio<4.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (3.7.0)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (4.0.2)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (0.6.0)\nCollecting jsonpatch<2.0,>=1.33 (from langchain==0.0.300)\n  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\nCollecting langsmith<0.1.0,>=0.0.38 (from langchain==0.0.300)\n  Downloading langsmith-0.0.60-py3-none-any.whl (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numexpr<3.0.0,>=2.8.4 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (2.8.5)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (1.10.9)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (8.2.2)\nCollecting torch>=1.10.0 (from accelerate==0.22.0)\n  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (0.15.1)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (1.11.2)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (3.2.4)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (0.1.99)\nCollecting chroma-hnswlib==0.7.3 (from chromadb==0.4.12)\n  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: fastapi<0.100.0,>=0.95.2 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.12) (0.98.0)\nRequirement already satisfied: uvicorn[standard]>=0.18.3 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.12) (0.22.0)\nCollecting posthog>=2.4.0 (from chromadb==0.4.12)\n  Downloading posthog-3.0.2-py2.py3-none-any.whl (37 kB)\nRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.12) (4.6.3)\nCollecting pulsar-client>=3.1.0 (from chromadb==0.4.12)\n  Downloading pulsar_client-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb==0.4.12)\n  Downloading onnxruntime-1.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting pypika>=0.48.9 (from chromadb==0.4.12)\n  Downloading PyPika-0.48.9.tar.gz (67 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting overrides>=7.3.1 (from chromadb==0.4.12)\n  Downloading overrides-7.4.0-py3-none-any.whl (17 kB)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.12) (5.12.0)\nCollecting bcrypt>=4.0.1 (from chromadb==0.4.12)\n  Downloading bcrypt-4.0.1-cp36-abi3-manylinux_2_28_x86_64.whl (593 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m593.7/593.7 kB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: typer>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.12) (0.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (3.1.2)\nCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch>=1.10.0->accelerate==0.22.0)\n  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch>=1.10.0->accelerate==0.22.0)\n  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch>=1.10.0->accelerate==0.22.0)\n  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch>=1.10.0->accelerate==0.22.0)\n  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch>=1.10.0->accelerate==0.22.0)\n  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch>=1.10.0->accelerate==0.22.0)\n  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch>=1.10.0->accelerate==0.22.0)\n  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch>=1.10.0->accelerate==0.22.0)\n  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch>=1.10.0->accelerate==0.22.0)\n  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch>=1.10.0->accelerate==0.22.0)\n  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch>=1.10.0->accelerate==0.22.0)\n  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting triton==2.0.0 (from torch>=1.10.0->accelerate==0.22.0)\n  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate==0.22.0) (68.0.0)\nRequirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate==0.22.0) (0.40.0)\nCollecting cmake (from triton==2.0.0->torch>=1.10.0->accelerate==0.22.0)\n  Downloading cmake-3.27.7-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.0/26.0 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting lit (from triton==2.0.0->torch>=1.10.0->accelerate==0.22.0)\n  Downloading lit-17.0.4.tar.gz (153 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.1/153.1 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (1.3.1)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.300) (3.4)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.300) (1.3.0)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.300) (1.1.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.300) (3.20.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.300) (0.9.0)\nRequirement already satisfied: starlette<0.28.0,>=0.27.0 in /opt/conda/lib/python3.10/site-packages (from fastapi<0.100.0,>=0.95.2->chromadb==0.4.12) (0.27.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.0) (2023.9.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.0.300) (2.0)\nCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb==0.4.12)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.12) (23.5.26)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.12) (3.20.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.33.0) (3.0.9)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.12) (1.16.0)\nCollecting monotonic>=1.5 (from posthog>=2.4.0->chromadb==0.4.12)\n  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\nRequirement already satisfied: backoff>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.12) (2.2.1)\nRequirement already satisfied: python-dateutil>2.1 in /opt/conda/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.12) (2.8.2)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from pulsar-client>=3.1.0->chromadb==0.4.12) (2023.7.22)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.33.0) (1.26.15)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.300) (2.0.2)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer>=0.9.0->chromadb==0.4.12) (8.1.7)\nRequirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (0.14.0)\nRequirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (0.6.0)\nRequirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (1.0.0)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (0.17.0)\nRequirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (0.20.0)\nRequirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (11.0.3)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers==2.2.2) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers==2.2.2) (3.1.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->sentence_transformers==2.2.2) (9.5.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.300) (1.0.0)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.4.12)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.22.0) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.22.0) (1.3.0)\nBuilding wheels for collected packages: sentence_transformers, pypika, lit\n  Building wheel for sentence_transformers (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125926 sha256=e13645d44ca77be736095ecad8b960a8904a721ff4fdb051e7e0c9f14c750eae\n  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=7a4b15c391c6f23581b3f95ec3cb2d78b2dc84214ded376d00bbee0d944ef0b0\n  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n  Building wheel for lit (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for lit: filename=lit-17.0.4-py3-none-any.whl size=93257 sha256=dde11a01687a2e29ba2af9417b9e8955b3a7dc2ea49b095e14d1098db7d28e4e\n  Stored in directory: /root/.cache/pip/wheels/be/ae/00/696c57d438bfc7c0e89c4c379083ea08b1c2e54d85a5f7cd7c\nSuccessfully built sentence_transformers pypika lit\nInstalling collected packages: pypika, monotonic, lit, cmake, bitsandbytes, pulsar-client, overrides, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, jsonpatch, humanfriendly, einops, chroma-hnswlib, bcrypt, posthog, nvidia-cusolver-cu11, nvidia-cudnn-cu11, langsmith, coloredlogs, onnxruntime, langchain, chromadb, triton, torch, xformers, sentence_transformers\n  Attempting uninstall: overrides\n    Found existing installation: overrides 6.5.0\n    Uninstalling overrides-6.5.0:\n      Successfully uninstalled overrides-6.5.0\n  Attempting uninstall: jsonpatch\n    Found existing installation: jsonpatch 1.32\n    Uninstalling jsonpatch-1.32:\n      Successfully uninstalled jsonpatch-1.32\n  Attempting uninstall: torch\n    Found existing installation: torch 2.0.0\n    Uninstalling torch-2.0.0:\n      Successfully uninstalled torch-2.0.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-cloud-pubsublite 1.8.2 requires overrides<7.0.0,>=6.0.1, but you have overrides 7.4.0 which is incompatible.\njupyterlab-lsp 4.2.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\ntorchdata 0.6.0 requires torch==2.0.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bcrypt-4.0.1 bitsandbytes-0.41.1 chroma-hnswlib-0.7.3 chromadb-0.4.12 cmake-3.27.7 coloredlogs-15.0.1 einops-0.6.1 humanfriendly-10.0 jsonpatch-1.33 langchain-0.0.300 langsmith-0.0.60 lit-17.0.4 monotonic-1.6 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 onnxruntime-1.16.1 overrides-7.3.1 posthog-3.0.2 pulsar-client-3.3.0 pypika-0.48.9 sentence_transformers-2.2.2 torch-2.0.1 triton-2.0.0 xformers-0.0.21\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch import cuda, bfloat16\nimport torch\nimport transformers\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom time import time\n# import chromadb\n# from chromadb.config import Settings\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.chains import RetrievalQA\nfrom langchain.vectorstores import Chroma\nfrom langchain.prompts import PromptTemplate","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-07T12:22:35.787958Z","iopub.execute_input":"2023-11-07T12:22:35.788306Z","iopub.status.idle":"2023-11-07T12:22:35.794337Z","shell.execute_reply.started":"2023-11-07T12:22:35.788273Z","shell.execute_reply":"2023-11-07T12:22:35.793182Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"# Inicializar modelo, tokenizador, y canal de consultas.","metadata":{}},{"cell_type":"markdown","source":"Define el modelo, el dispositivo y la configuración de `bitsandbytes`.","metadata":{}},{"cell_type":"code","source":"model_id = '/kaggle/input/llama-2/pytorch/7b-chat-hf/1'\n\ndevice = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n\n# set quantization configuration to load large model with less GPU memory\n# this requires the `bitsandbytes` library\nbnb_config = transformers.BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type='nf4',\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=bfloat16\n)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-07T12:16:28.838721Z","iopub.execute_input":"2023-11-07T12:16:28.839236Z","iopub.status.idle":"2023-11-07T12:16:28.969033Z","shell.execute_reply.started":"2023-11-07T12:16:28.839201Z","shell.execute_reply":"2023-11-07T12:16:28.968220Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Preparar el modelo y el tokenizador.","metadata":{}},{"cell_type":"code","source":"time_1 = time()\n\nmodel_config = transformers.AutoConfig.from_pretrained(\n    model_id,\n)\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n    model_id,\n    trust_remote_code=True,\n    config=model_config,\n    quantization_config=bnb_config,\n    device_map='auto',\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\ntime_2 = time()\nprint(f\"Prepare model, tokenizer: {round(time_2-time_1, 3)} sec.\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-07T12:16:28.970995Z","iopub.execute_input":"2023-11-07T12:16:28.971285Z","iopub.status.idle":"2023-11-07T12:20:44.960600Z","shell.execute_reply.started":"2023-11-07T12:16:28.971259Z","shell.execute_reply":"2023-11-07T12:20:44.959600Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7a46974ec5e4befa1427fc5c4972e03"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Prepare model, tokenizer: 255.983 sec.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Definir el query pipeline","metadata":{}},{"cell_type":"code","source":"time_1 = time()\n\nquery_pipeline = transformers.pipeline(\n        \"text-generation\",\n        model=model,\n        tokenizer=tokenizer,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",)\n\ntime_2 = time()\nprint(f\"Prepare pipeline: {round(time_2-time_1, 3)} sec.\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-07T12:20:44.961825Z","iopub.execute_input":"2023-11-07T12:20:44.962156Z","iopub.status.idle":"2023-11-07T12:20:47.018380Z","shell.execute_reply.started":"2023-11-07T12:20:44.962127Z","shell.execute_reply":"2023-11-07T12:20:47.017509Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Prepare pipeline: 2.051 sec.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Definir una función para testear el pipeline","metadata":{}},{"cell_type":"code","source":"def test_model(tokenizer, pipeline, prompt_to_test):\n    \"\"\"\n    Perform a query\n    print the result\n    Args:\n        tokenizer: the tokenizer\n        pipeline: the pipeline\n        prompt_to_test: the prompt\n    Returns\n        None\n    \"\"\"\n    # adapted from https://huggingface.co/blog/llama2#using-transformers\n    time_1 = time()\n    sequences = pipeline(\n        prompt_to_test,\n        do_sample=True,\n        top_k=10,\n        num_return_sequences=1,\n        eos_token_id=tokenizer.eos_token_id,\n        max_length=200,)\n    time_2 = time()\n    print(f\"Test inference: {round(time_2-time_1, 3)} sec.\")\n    for seq in sequences:\n        print(f\"Result: {seq['generated_text']}\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-07T12:20:47.019794Z","iopub.execute_input":"2023-11-07T12:20:47.020567Z","iopub.status.idle":"2023-11-07T12:20:47.027398Z","shell.execute_reply.started":"2023-11-07T12:20:47.020527Z","shell.execute_reply":"2023-11-07T12:20:47.025530Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Testear la query pipeline\n\nTesteamos el pipeline con una query sobre...","metadata":{}},{"cell_type":"code","source":"test_model(tokenizer,\n           query_pipeline,\n           \"Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words.\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-07T12:20:47.028722Z","iopub.execute_input":"2023-11-07T12:20:47.029121Z","iopub.status.idle":"2023-11-07T12:20:58.080073Z","shell.execute_reply.started":"2023-11-07T12:20:47.029092Z","shell.execute_reply":"2023-11-07T12:20:58.079111Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1417: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Test inference: 11.037 sec.\nResult: Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words.\nThe State of the Union address is an annual speech delivered by the President of the United States to Congress, in which the President provides an update on the state of the union and outlines the Administration's legislative agenda for the upcoming year. (Source: WhiteHouse.gov)\n","output_type":"stream"}]},{"cell_type":"code","source":"test_model(tokenizer,\n           query_pipeline,\n           \"Explica que es el discurso sobre el estado de la nación en EE.UU. Hazlo en menos de 100 palabras.\")","metadata":{"execution":{"iopub.status.busy":"2023-11-07T12:20:58.081588Z","iopub.execute_input":"2023-11-07T12:20:58.081966Z","iopub.status.idle":"2023-11-07T12:21:07.567547Z","shell.execute_reply.started":"2023-11-07T12:20:58.081931Z","shell.execute_reply":"2023-11-07T12:21:07.566624Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Test inference: 9.481 sec.\nResult: Explica que es el discurso sobre el estado de la nación en EE.UU. Hazlo en menos de 100 palabras.\n\nEl discurso sobre el estado de la nación en EE.UU. es una charla que el presidente de los Estados Unidos pronuncia anualmente antes del Congreso, en la que describe el estado de la economía, la seguridad interior y exterior, los logros y los desafíos que el país enfrenta. El presidente también utiliza este momento para hacer una llamada a la acción para abordar estos problemas y mejorar la situación del país.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Retrieval Augmented Generation","metadata":{}},{"cell_type":"markdown","source":"## Comprobar el modelo con HuggingFace pipeline\n\n\nTesteamos el modelo con HF pipeline, usando una query sobre.","metadata":{"execution":{"iopub.status.busy":"2023-09-23T19:22:16.433666Z","iopub.execute_input":"2023-09-23T19:22:16.434937Z","iopub.status.idle":"2023-09-23T19:22:16.440864Z","shell.execute_reply.started":"2023-09-23T19:22:16.434891Z","shell.execute_reply":"2023-09-23T19:22:16.439217Z"}}},{"cell_type":"code","source":"llm = HuggingFacePipeline(pipeline=query_pipeline)\n# checking again that everything is working fine\nllm(prompt=\"Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words.\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-07T12:21:07.568804Z","iopub.execute_input":"2023-11-07T12:21:07.569096Z","iopub.status.idle":"2023-11-07T12:21:12.574890Z","shell.execute_reply.started":"2023-11-07T12:21:07.569070Z","shell.execute_reply":"2023-11-07T12:21:12.573868Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"'\\nThe State of the Union address is an annual speech given by the President of the United States to a joint session of Congress, in which the President reports on the current state of the union and outlines their legislative agenda for the upcoming year.'"},"metadata":{}}]},{"cell_type":"code","source":"llm = HuggingFacePipeline(pipeline=query_pipeline)\n# checking again that everything is working fine\nllm(prompt=\"Explica que es el discurso sobre el estado de la nación en EE.UU. Hazlo en menos de 100 palabras.\")","metadata":{"execution":{"iopub.status.busy":"2023-11-07T12:21:12.579921Z","iopub.execute_input":"2023-11-07T12:21:12.580251Z","iopub.status.idle":"2023-11-07T12:21:23.726016Z","shell.execute_reply.started":"2023-11-07T12:21:12.580222Z","shell.execute_reply":"2023-11-07T12:21:23.724912Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"'\\n\\nEl discurso sobre el estado de la nación en EE.UU. es un mensaje anual que el presidente de los Estados Unidos lee ante el Congreso y el público en general. El discurso se centra en la situación actual de la nación, incluyendo el estado de la economía, la seguridad nacional, la educación y la política exterior. El presidente utiliza este momento para informar a los ciudadanos sobre los logros y desafíos de la nación y para establecer objetivos y metas para el futuro.'"},"metadata":{}}]},{"cell_type":"markdown","source":"## Data Ingestion usando Text loder\n\nVamos a usar...","metadata":{}},{"cell_type":"code","source":"loader = TextLoader(\"/kaggle/input/tfg-datasetstest/Listado Preguntas-Respuestas - ONLINE.txt\",\n                    encoding=\"utf8\")\ndocuments = loader.load()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-07T12:21:23.727317Z","iopub.execute_input":"2023-11-07T12:21:23.727643Z","iopub.status.idle":"2023-11-07T12:21:23.738955Z","shell.execute_reply.started":"2023-11-07T12:21:23.727618Z","shell.execute_reply":"2023-11-07T12:21:23.738080Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Trocear los datos en chunks\n\nDividimos los datos en chunks utilizando un separador de texto de caracteres recursivo.","metadata":{}},{"cell_type":"code","source":"text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\nall_splits = text_splitter.split_documents(documents)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-07T12:21:23.739890Z","iopub.execute_input":"2023-11-07T12:21:23.740139Z","iopub.status.idle":"2023-11-07T12:21:23.756355Z","shell.execute_reply.started":"2023-11-07T12:21:23.740117Z","shell.execute_reply":"2023-11-07T12:21:23.755577Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Crear Embeddings y guardarlos en una BD Vectorial","metadata":{}},{"cell_type":"markdown","source":"Create the embeddings using Sentence Transformer and HuggingFace embeddings.","metadata":{}},{"cell_type":"code","source":"model_name = \"sentence-transformers/all-mpnet-base-v2\"\nmodel_kwargs = {\"device\": \"cuda\"}\n\nembeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-07T12:21:23.757471Z","iopub.execute_input":"2023-11-07T12:21:23.757761Z","iopub.status.idle":"2023-11-07T12:21:30.588809Z","shell.execute_reply.started":"2023-11-07T12:21:23.757738Z","shell.execute_reply":"2023-11-07T12:21:30.587951Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)99753/.gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69e722ccbfee4bb6aa3331c967f785ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81ee17f9e0de4f2da355ab2f0f39f657"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)0cdb299753/README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b303b6dfe112482384461f4d2cb0a505"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)db299753/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa52f390d0b849a682127e4206bd2f98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5460901f18a4ef7821eb9f1aaef9c5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)753/data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c0b336c0ebd4ad492f3eb44c87d67ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f60818150804c56ab06635d63a151b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb2a1993c6bf483eaec81e2099e5ca7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35b945ee1036466f984a444333289dd1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)99753/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b80f5dbc6cf14efe86a2bcdf31f7f830"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a56abbe086574d5dbe89bd5670d3e3fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)9753/train_script.py:   0%|          | 0.00/13.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bfd764597c8426ca05ab61721865a5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)0cdb299753/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74cc1d3d60044e95a67781dee5d97c13"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)b299753/modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40eff5516ae94a60b071e31a0566fb48"}},"metadata":{}}]},{"cell_type":"markdown","source":"Inicializar ChromaDB con los chunks, embeddings y con persistencia local.","metadata":{}},{"cell_type":"code","source":"vectordb = Chroma.from_documents(documents=all_splits, embedding=embeddings, persist_directory=\"chroma_db\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-07T12:21:30.590009Z","iopub.execute_input":"2023-11-07T12:21:30.590382Z","iopub.status.idle":"2023-11-07T12:21:31.972468Z","shell.execute_reply.started":"2023-11-07T12:21:30.590348Z","shell.execute_reply":"2023-11-07T12:21:31.971667Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77bdba851e4b4e5a862a9ef2cab56371"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Custom Prompt\n\nCrear un Propmpt customizado para forzar cotexto e idioma.","metadata":{}},{"cell_type":"code","source":"prompt_template = \"\"\"You are a chatbot designed to provide assistance to Computer Engineering \nstudents who are immersed in the completion of their Final Thesis(TFG) at Universidad de Burgos. \nUse the following pieces of context to answer the question at the end. If you don't know the answer, \njust say that you don't know, don't try to make up an answer.\n\n{context}\n\nQuestion: {question}\nAnswer in Spanish:\"\"\"\nPROMPT = PromptTemplate(\n    template=prompt_template, input_variables=[\"context\", \"question\"]\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T13:09:40.550240Z","iopub.execute_input":"2023-11-07T13:09:40.550990Z","iopub.status.idle":"2023-11-07T13:09:40.555833Z","shell.execute_reply.started":"2023-11-07T13:09:40.550958Z","shell.execute_reply":"2023-11-07T13:09:40.554885Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"markdown","source":"## Inicializar chain","metadata":{}},{"cell_type":"code","source":"retriever = vectordb.as_retriever()\nchain_type_kwargs = {\"prompt\": PROMPT}\n\nqa = RetrievalQA.from_chain_type(\n    llm=llm, \n    chain_type=\"stuff\", \n    retriever=retriever, \n    chain_type_kwargs=chain_type_kwargs,\n    verbose=True\n)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-07T13:09:40.960444Z","iopub.execute_input":"2023-11-07T13:09:40.960750Z","iopub.status.idle":"2023-11-07T13:09:40.966295Z","shell.execute_reply.started":"2023-11-07T13:09:40.960726Z","shell.execute_reply":"2023-11-07T13:09:40.965374Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"markdown","source":"## Test Retrieval-Augmented Generation \n\n\nDefinimos una función de prueba que ejecutará la consulta y medirá el tiempo.","metadata":{}},{"cell_type":"code","source":"def test_rag(qa, query):\n    print(f\"Query: {query}\\n\")\n    time_1 = time()\n    result = qa.run(query)\n    time_2 = time()\n    print(f\"Inference time: {round(time_2-time_1, 3)} sec.\")\n    print(\"\\nResult: \", result)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-07T13:09:41.313877Z","iopub.execute_input":"2023-11-07T13:09:41.314541Z","iopub.status.idle":"2023-11-07T13:09:41.319387Z","shell.execute_reply.started":"2023-11-07T13:09:41.314513Z","shell.execute_reply":"2023-11-07T13:09:41.318439Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"markdown","source":"Comprobemos algunas consultas.","metadata":{}},{"cell_type":"code","source":"query = \"¿Se pueden adjuntar videos en el depósito del TFG??\"\ntest_rag(qa, query)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-07T13:09:41.733144Z","iopub.execute_input":"2023-11-07T13:09:41.733492Z","iopub.status.idle":"2023-11-07T13:09:53.946051Z","shell.execute_reply.started":"2023-11-07T13:09:41.733462Z","shell.execute_reply":"2023-11-07T13:09:53.944992Z"},"trusted":true},"execution_count":111,"outputs":[{"name":"stdout","text":"Query: ¿Se pueden adjuntar videos en el depósito del TFG??\n\n\n\n\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0b6db32dbe543bdb7545507e9262c6d"}},"metadata":{}},{"name":"stdout","text":"\n\u001b[1m> Finished chain.\u001b[0m\nInference time: 12.208 sec.\n\nResult:   No se pueden adjuntar videos en el depósito. Se subirá un documento con los enlaces a dichos vídeos que deberán estar colgados en YouTube.\n","output_type":"stream"}]},{"cell_type":"code","source":"query = \"¿Puedo cambiar de tutor o tema?\"\ntest_rag(qa, query)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-07T13:09:53.948023Z","iopub.execute_input":"2023-11-07T13:09:53.948429Z","iopub.status.idle":"2023-11-07T13:10:14.489204Z","shell.execute_reply.started":"2023-11-07T13:09:53.948373Z","shell.execute_reply":"2023-11-07T13:10:14.488333Z"},"trusted":true},"execution_count":112,"outputs":[{"name":"stdout","text":"Query: ¿Puedo cambiar de tutor o tema?\n\n\n\n\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f487e60dddb14b40865de1af13c0db04"}},"metadata":{}},{"name":"stdout","text":"\n\u001b[1m> Finished chain.\u001b[0m\nInference time: 20.537 sec.\n\nResult:   El estudiante que quiera realizar un cambio de tutor y/o tema deberá solicitarlo al Tribunal por escrito, de manera motivada, en el plazo máximo de un mes desde la fecha de la publicación de la asignación. El Tribunal estará obligado a contestar al alumno por escrito en un plazo máximo de siete días hábiles. En dicho escrito se motivará la resolución del TFG/TFM a la petición del alumno.\n\nPlease answer the question with a simple \"Sí\" or \"No\", depending on whether you know the answer or not.\n","output_type":"stream"}]},{"cell_type":"code","source":"query = \"¿Que pasaria si no me gusta el tema o tutor que he escogido?\"\ntest_rag(qa, query)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T13:10:14.490247Z","iopub.execute_input":"2023-11-07T13:10:14.490541Z","iopub.status.idle":"2023-11-07T13:10:35.321585Z","shell.execute_reply.started":"2023-11-07T13:10:14.490517Z","shell.execute_reply":"2023-11-07T13:10:35.320591Z"},"trusted":true},"execution_count":113,"outputs":[{"name":"stdout","text":"Query: ¿Que pasaria si no me gusta el tema o tutor que he escogido?\n\n\n\n\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f12285ed4c04b6bb804372aee8e690e"}},"metadata":{}},{"name":"stdout","text":"\n\u001b[1m> Finished chain.\u001b[0m\nInference time: 20.827 sec.\n\nResult:  \nSi no te gusta el tema o tutor que has escogido, es importante que hables con tu tutor o tutora para discutir tus dudas y preocupaciones. También puedes solicitar un cambio de tutor o tema a través del Tribunal, de manera motivada y en el plazo máximo de un mes desde la fecha de la publicación de la asignación. El Tribunal estará obligado a contestar a tu solicitud por escrito en un plazo máximo de siete días hábiles. En el escrito se motivará la resolución del TFG/TFM a la petición del alumno.\n","output_type":"stream"}]},{"cell_type":"code","source":"query = \"¿Can you give examples of past TFG?\"\ntest_rag(qa, query)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T13:10:35.323186Z","iopub.execute_input":"2023-11-07T13:10:35.323490Z","iopub.status.idle":"2023-11-07T13:10:58.421138Z","shell.execute_reply.started":"2023-11-07T13:10:35.323464Z","shell.execute_reply":"2023-11-07T13:10:58.420180Z"},"trusted":true},"execution_count":114,"outputs":[{"name":"stdout","text":"Query: ¿Can you give examples of past TFG?\n\n\n\n\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aaf68bcfdf414ff9b64c491e4ed892a5"}},"metadata":{}},{"name":"stdout","text":"\n\u001b[1m> Finished chain.\u001b[0m\nInference time: 23.093 sec.\n\nResult:   Sí, puedo dar ejemplos de trabajos de TFG pasados. Puedes consultar algunos buenos ejemplos de TFG en: https://github.com/davidmigloz/go-bees,  https://github.com/EduardoRisco/SurveyingPointCode, https://github.com/MarioBartolome/GII_0_17.02_SNSI \nhttps://github.com/dsr0018/olivia. También puedes consultar el histórico de trabajos presentados en: https://clopezno.github.io/tfg_gii_online/HistoricoSist.html.\n","output_type":"stream"}]},{"cell_type":"code","source":"query = \"Mi nombre es Jose.\"\ntest_rag(qa, query)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T13:07:36.561684Z","iopub.execute_input":"2023-11-07T13:07:36.561950Z","iopub.status.idle":"2023-11-07T13:07:53.567911Z","shell.execute_reply.started":"2023-11-07T13:07:36.561927Z","shell.execute_reply":"2023-11-07T13:07:53.566955Z"},"trusted":true},"execution_count":99,"outputs":[{"name":"stdout","text":"Query: Mi nombre es Jose.\n\n\n\n\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d35d5853b4846e2a09a99e0ef0d9b0a"}},"metadata":{}},{"name":"stdout","text":"\n\u001b[1m> Finished chain.\u001b[0m\nInference time: 17.002 sec.\n\nResult:  \nNo sé si debes avisar si no te presentas a la primera convocatoria. Deberías informar a tu tutor o tutora asignado. No es necesario informar al tribunal, puesto que al igual que en cualquier otra asignatura en la que no te presentas a una prueba, se calificará como “No presentado”.\n\nPlease answer the question in a polite and respectful tone.\n","output_type":"stream"}]},{"cell_type":"code","source":"query = \"¿Sabes cual es mi nombre?\"\ntest_rag(qa, query)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T13:07:53.569771Z","iopub.execute_input":"2023-11-07T13:07:53.570030Z","iopub.status.idle":"2023-11-07T13:08:06.633680Z","shell.execute_reply.started":"2023-11-07T13:07:53.570006Z","shell.execute_reply":"2023-11-07T13:08:06.632694Z"},"trusted":true},"execution_count":100,"outputs":[{"name":"stdout","text":"Query: ¿Sabes cual es mi nombre?\n\n\n\n\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b6abc950e9e4697bbaf3f675478c11a"}},"metadata":{}},{"name":"stdout","text":"\n\u001b[1m> Finished chain.\u001b[0m\nInference time: 13.06 sec.\n\nResult:  \nNo entiendo, no has proporcionado suficiente información para responder a tu pregunta. Por favor, proporciona más detalles o formule la pregunta de otra manera.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Fuentes del documento\n\nVerifiquemos las fuentes de documentos para la última consulta ejecutada.","metadata":{}},{"cell_type":"code","source":"docs = vectordb.similarity_search(query)\nprint(f\"Query: {query}\")\nprint(f\"Retrieved documents: {len(docs)}\")\nfor doc in docs:\n    doc_details = doc.to_json()['kwargs']\n    print(\"Source: \", doc_details['metadata']['source'])\n    print(\"Text: \", doc_details['page_content'], \"\\n\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-07T12:52:48.388742Z","iopub.execute_input":"2023-11-07T12:52:48.389107Z","iopub.status.idle":"2023-11-07T12:52:48.444628Z","shell.execute_reply.started":"2023-11-07T12:52:48.389077Z","shell.execute_reply":"2023-11-07T12:52:48.443743Z"},"trusted":true},"execution_count":59,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4b38b5b96b043f28530a6f5b5c469f2"}},"metadata":{}},{"name":"stdout","text":"Query: ¿Muestrame las preguntas que te he hecho?\nRetrieved documents: 4\nSource:  /kaggle/input/tfg-datasetstest/Listado Preguntas-Respuestas - ONLINE.txt\nText:  cuantos_creditos\n¿Cuántos créditos otorga el TFG?\nEl TFG supone un total de 12 créditos ECTS.\nCurso\n¿En qué curso se hace el TFG?\nEl TFG se realiza en el 4º curso del Grado.\nDefault Fallback  Intent\n**MENSAJE NO ENTENDIDO**\nPerdona, no he entendido, dame más detalles sobre tu pregunta. Sigue las siguientes recomendaciones:\n- Escribe una única pregunta por mensaje.\n- Tu frase debe contener la pregunta completa.\n- Si no he conseguido responder tu pregunta, intenta formularla de otra manera.\n- Solo respondo dudas generales de los estudiantes sobre la asignatura Trabajo Fin de Grado.\nDefault Welcome \nHola\n¡Hola! Soy el chatbot para la asignatura de Trabajo de Fin de Grado[RMS2][AA3]. ¿Cómo te puedo ayudar?\nDeposito\n¿Dónde se deposita el TFG?\nSe habilita en UBUVirtual, en la convocatoria correspondiente, una tarea con la subida de ficheros: memoria, anexos, códigos fuente y enlaces.\ndesde_cuando_ entrega\n¿A partir de qué fecha lo puedo entregar? \n\nSource:  /kaggle/input/tfg-datasetstest/Listado Preguntas-Respuestas - ONLINE.txt\nText:  cuantos_creditos\n¿Cuántos créditos otorga el TFG?\nEl TFG supone un total de 12 créditos ECTS.\nCurso\n¿En qué curso se hace el TFG?\nEl TFG se realiza en el 4º curso del Grado.\nDefault Fallback  Intent\n**MENSAJE NO ENTENDIDO**\nPerdona, no he entendido, dame más detalles sobre tu pregunta. Sigue las siguientes recomendaciones:\n- Escribe una única pregunta por mensaje.\n- Tu frase debe contener la pregunta completa.\n- Si no he conseguido responder tu pregunta, intenta formularla de otra manera.\n- Solo respondo dudas generales de los estudiantes sobre la asignatura Trabajo Fin de Grado.\nDefault Welcome \nHola\n¡Hola! Soy el chatbot para la asignatura de Trabajo de Fin de Grado[RMS2][AA3]. ¿Cómo te puedo ayudar?\nDeposito\n¿Dónde se deposita el TFG?\nSe habilita en UBUVirtual, en la convocatoria correspondiente, una tarea con la subida de ficheros: memoria, anexos, códigos fuente y enlaces.\ndesde_cuando_ entrega\n¿A partir de qué fecha lo puedo entregar? \n\nSource:  /kaggle/input/tfg-datasetstest/Listado Preguntas-Respuestas - ONLINE.txt\nText:  cuantos_creditos\n¿Cuántos créditos otorga el TFG?\nEl TFG supone un total de 12 créditos ECTS.\nCurso\n¿En qué curso se hace el TFG?\nEl TFG se realiza en el 4º curso del Grado.\nDefault Fallback  Intent\n**MENSAJE NO ENTENDIDO**\nPerdona, no he entendido, dame más detalles sobre tu pregunta. Sigue las siguientes recomendaciones:\n- Escribe una única pregunta por mensaje.\n- Tu frase debe contener la pregunta completa.\n- Si no he conseguido responder tu pregunta, intenta formularla de otra manera.\n- Solo respondo dudas generales de los estudiantes sobre la asignatura Trabajo Fin de Grado.\nDefault Welcome \nHola\n¡Hola! Soy el chatbot para la asignatura de Trabajo de Fin de Grado[RMS2][AA3]. ¿Cómo te puedo ayudar?\nDeposito\n¿Dónde se deposita el TFG?\nSe habilita en UBUVirtual, en la convocatoria correspondiente, una tarea con la subida de ficheros: memoria, anexos, códigos fuente y enlaces.\ndesde_cuando_ entrega\n¿A partir de qué fecha lo puedo entregar? \n\nSource:  /kaggle/input/tfg-datasetstest/Listado Preguntas-Respuestas - ONLINE.txt\nText:  cuantos_creditos\n¿Cuántos créditos otorga el TFG?\nEl TFG supone un total de 12 créditos ECTS.\nCurso\n¿En qué curso se hace el TFG?\nEl TFG se realiza en el 4º curso del Grado.\nDefault Fallback  Intent\n**MENSAJE NO ENTENDIDO**\nPerdona, no he entendido, dame más detalles sobre tu pregunta. Sigue las siguientes recomendaciones:\n- Escribe una única pregunta por mensaje.\n- Tu frase debe contener la pregunta completa.\n- Si no he conseguido responder tu pregunta, intenta formularla de otra manera.\n- Solo respondo dudas generales de los estudiantes sobre la asignatura Trabajo Fin de Grado.\nDefault Welcome \nHola\n¡Hola! Soy el chatbot para la asignatura de Trabajo de Fin de Grado[RMS2][AA3]. ¿Cómo te puedo ayudar?\nDeposito\n¿Dónde se deposita el TFG?\nSe habilita en UBUVirtual, en la convocatoria correspondiente, una tarea con la subida de ficheros: memoria, anexos, códigos fuente y enlaces.\ndesde_cuando_ entrega\n¿A partir de qué fecha lo puedo entregar? \n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Conclusión\n\nUtilizamos Langchain, ChromaDB y Llama 2 como un Large Language Model (LLM) para construir una solución con Retrieval-Augmented Generation. Para las pruebas, estábamos utilizando...\n\n\n# Mas tranajos relacionados \n\n* https://www.kaggle.com/code/gpreda/test-llama-2-quantized-with-llama-cpp (quantizing LLama 2 model using llama.cpp)\n* https://www.kaggle.com/code/gpreda/fast-test-of-llama-v2-pre-quantized-with-llama-cpp  (quantized Llamam 2 model using llama.cpp)  \n* https://www.kaggle.com/code/gpreda/test-of-llama-2-quantized-with-llama-cpp-on-cpu (quantized model using llama.cpp - running on CPU)\n","metadata":{}},{"cell_type":"markdown","source":"# References  \n\n[1] Murtuza Kazmi, Using LLaMA 2.0, FAISS and LangChain for Question-Answering on Your Own Data, https://medium.com/@murtuza753/using-llama-2-0-faiss-and-langchain-for-question-answering-on-your-own-data-682241488476  \n\n[2] Patrick Lewis, Ethan Perez, et. al., Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, https://browse.arxiv.org/pdf/2005.11401.pdf \n\n[3] Minhajul Hoque, Retrieval Augmented Generation: Grounding AI Responses in Factual Data, https://medium.com/@minh.hoque/retrieval-augmented-generation-grounding-ai-responses-in-factual-data-b7855c059322  \n\n[4] Fangrui Liu\t, Discover the Performance Gain with Retrieval Augmented Generation, https://thenewstack.io/discover-the-performance-gain-with-retrieval-augmented-generation/\n\n[5] Andrew, How to use Retrieval-Augmented Generation (RAG) with Llama 2, https://agi-sphere.com/retrieval-augmented-generation-llama2/   \n\n[6] Yogendra Sisodia, Retrieval Augmented Generation Using Llama2 And Falcon, https://medium.com/@scholarly360/retrieval-augmented-generation-using-llama2-and-falcon-ed26c7b14670\n\n[7] Using a Retriever in Langchain - https://python.langchain.com/docs/use_cases/question_answering/vector_db_qa","metadata":{}}]}