{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introducción\n\n\n## Objetivo\n\nUtilizar Llama 2.0, Langchain y ChromaDB para crear un sistema de Generación con Recuperación Mejorada (RAG). Esto nos permitirá hacer preguntas sobre nuestros documentos (que no se incluyeron en los datos de entrenamiento), sin necesidad de ajustar finamente el Modelo de Lenguaje Grande (LLM, por sus siglas en inglés).\nCuando se utiliza RAG, si se plantea una pregunta, primero se realiza un paso de recuperación para obtener documentos relevantes de una base de datos especial, una base de datos vectorial donde se indexaron estos documentos.\n\n## Definiciones\n\n* LLM - Modelo de Lenguaje Grande (Large Language Model)\n* Llama 2.0 - LLM de Meta\n* Langchain - un marco diseñado para simplificar la creación de aplicaciones utilizando LLM\n* Base de datos vectorial - una base de datos que organiza datos a través de vectores de alta dimensión\n* ChromaDB - base de datos vectorial\n* RAG - Generación con Recuperación Mejorada (consulte más detalles sobre RAG a continuación)\n\n## Detalles del modelo\n\n* **Modelo**: Llama 2\n* **Variante**: 7b-chat-hf (7b: 7 mil millones de dimensiones, hf: compilación de HuggingFace)\n* **Versión**: V1\n* **Framework**: PyTorch\n\nEl modelo LlaMA 2 está preentrenado y ajustado con 2 billones de tokens y de 7 a 70 mil millones de parámetros, lo que lo convierte en uno de los modelos de código abierto más potentes. Es una mejora significativa con respecto al modelo LlaMA 1.\n\n\n## ¿Qué es un sistema de Generación con Recuperación Mejorada (RAG)?\n\nLos Modelos de Lenguaje Grande (LLM) han demostrado su capacidad para comprender el contexto y proporcionar respuestas precisas a diversas tareas de Procesamiento de Lenguaje Natural (NLP), incluyendo la resumen, preguntas y respuestas, cuando se les solicita. Si bien son capaces de proporcionar respuestas muy buenas a preguntas sobre información con la que fueron entrenados, tienden a alucinar cuando el tema trata sobre información que \"no saben\", es decir, no estaba incluida en sus datos de entrenamiento. La Generación con Recuperación Mejorada combina recursos externos con LLM. Por lo tanto, los dos componentes principales de un sistema RAG son un recuperador y un generador.\n\nLa parte del recuperador se puede describir como un sistema que es capaz de codificar nuestros datos para que se puedan recuperar fácilmente las partes relevantes al consultarlos. La codificación se realiza utilizando incrustaciones de texto, es decir, un modelo entrenado para crear una representación vectorial de la información. La mejor opción para implementar un recuperador es una base de datos vectoriales. Como bases de datos vectoriales, existen múltiples opciones, tanto productos de código abierto como comerciales. Algunos ejemplos son ChromaDB, Mevius, FAISS, Pinecone, Weaviate. Nuestra opción en este cuaderno será una instancia local de ChromaDB (persistente).\n\nPara la parte del generador, la opción más obvia es un LLM. En este cuaderno utilizaremos un modelo LLaMA v2 cuantificado, de la colección de Modelos de Kaggle.\n\nLa orquestación del recuperador y el generador se realizará utilizando Langchain. Una función especializada de Langchain nos permite crear el recuperador-generador en una sola línea de código.\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Installations, imports, utils","metadata":{}},{"cell_type":"code","source":"!pip install transformers==4.33.0 accelerate==0.22.0 einops==0.6.1 langchain==0.0.300 xformers==0.0.21 \\\nbitsandbytes==0.41.1 sentence_transformers==2.2.2 chromadb==0.4.12","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-11-01T13:11:51.691416Z","iopub.execute_input":"2023-11-01T13:11:51.691736Z","iopub.status.idle":"2023-11-01T13:14:56.712711Z","shell.execute_reply.started":"2023-11-01T13:11:51.691708Z","shell.execute_reply":"2023-11-01T13:14:56.711720Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U transformers accelerate einops langchain xformers bitsandbytes sentence_transformers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import cuda, bfloat16\nimport torch\nimport transformers\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom time import time\n# import chromadb\n# from chromadb.config import Settings\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.chains import RetrievalQA\nfrom langchain.vectorstores import Chroma\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-01T13:14:56.717990Z","iopub.execute_input":"2023-11-01T13:14:56.718363Z","iopub.status.idle":"2023-11-01T13:15:02.210942Z","shell.execute_reply.started":"2023-11-01T13:14:56.718325Z","shell.execute_reply":"2023-11-01T13:15:02.210022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inicializar modelo, tokenizador, y canal de consultas.","metadata":{}},{"cell_type":"markdown","source":"Define el modelo, el dispositivo y la configuración de `bitsandbytes`.","metadata":{}},{"cell_type":"code","source":"model_id = '/kaggle/input/llama-2/pytorch/7b-chat-hf/1'\n\ndevice = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n\n# set quantization configuration to load large model with less GPU memory\n# this requires the `bitsandbytes` library\nbnb_config = transformers.BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type='nf4',\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=bfloat16\n)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-01T13:15:02.212155Z","iopub.execute_input":"2023-11-01T13:15:02.212581Z","iopub.status.idle":"2023-11-01T13:15:02.335953Z","shell.execute_reply.started":"2023-11-01T13:15:02.212552Z","shell.execute_reply":"2023-11-01T13:15:02.335079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Preparar el modelo y el tokenizador.","metadata":{}},{"cell_type":"code","source":"time_1 = time()\n\nmodel_config = transformers.AutoConfig.from_pretrained(\n    model_id,\n)\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n    model_id,\n    trust_remote_code=True,\n    config=model_config,\n    quantization_config=bnb_config,\n    device_map='auto',\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\ntime_2 = time()\nprint(f\"Prepare model, tokenizer: {round(time_2-time_1, 3)} sec.\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-01T13:15:02.337946Z","iopub.execute_input":"2023-11-01T13:15:02.338263Z","iopub.status.idle":"2023-11-01T13:17:59.512152Z","shell.execute_reply.started":"2023-11-01T13:15:02.338237Z","shell.execute_reply":"2023-11-01T13:17:59.511139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Definir el query pipeline","metadata":{}},{"cell_type":"code","source":"time_1 = time()\n\nquery_pipeline = transformers.pipeline(\n        \"text-generation\",\n        model=model,\n        tokenizer=tokenizer,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",)\n\ntime_2 = time()\nprint(f\"Prepare pipeline: {round(time_2-time_1, 3)} sec.\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-01T13:18:06.326644Z","iopub.execute_input":"2023-11-01T13:18:06.327042Z","iopub.status.idle":"2023-11-01T13:18:08.260722Z","shell.execute_reply.started":"2023-11-01T13:18:06.327007Z","shell.execute_reply":"2023-11-01T13:18:08.259620Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Definir una función para testear el pipeline","metadata":{}},{"cell_type":"code","source":"def test_model(tokenizer, pipeline, prompt_to_test):\n    \"\"\"\n    Perform a query\n    print the result\n    Args:\n        tokenizer: the tokenizer\n        pipeline: the pipeline\n        prompt_to_test: the prompt\n    Returns\n        None\n    \"\"\"\n    # adapted from https://huggingface.co/blog/llama2#using-transformers\n    time_1 = time()\n    sequences = pipeline(\n        prompt_to_test,\n        do_sample=True,\n        top_k=10,\n        num_return_sequences=1,\n        eos_token_id=tokenizer.eos_token_id,\n        max_length=200,)\n    time_2 = time()\n    print(f\"Test inference: {round(time_2-time_1, 3)} sec.\")\n    for seq in sequences:\n        print(f\"Result: {seq['generated_text']}\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-01T13:18:11.944797Z","iopub.execute_input":"2023-11-01T13:18:11.945608Z","iopub.status.idle":"2023-11-01T13:18:11.951874Z","shell.execute_reply.started":"2023-11-01T13:18:11.945566Z","shell.execute_reply":"2023-11-01T13:18:11.950789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Testear la query pipeline\n\nTesteamos el pipeline con una query sobre...","metadata":{}},{"cell_type":"code","source":"test_model(tokenizer,\n           query_pipeline,\n           \"Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words.\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-01T13:18:14.234465Z","iopub.execute_input":"2023-11-01T13:18:14.235301Z","iopub.status.idle":"2023-11-01T13:18:23.051020Z","shell.execute_reply.started":"2023-11-01T13:18:14.235260Z","shell.execute_reply":"2023-11-01T13:18:23.050054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_model(tokenizer,\n           query_pipeline,\n           \"Explica que es el discurso sobre el estado de la nación en EE.UU. Hazlo en menos de 100 palabras.\")","metadata":{"execution":{"iopub.status.busy":"2023-11-01T13:18:26.849734Z","iopub.execute_input":"2023-11-01T13:18:26.850495Z","iopub.status.idle":"2023-11-01T13:18:38.826040Z","shell.execute_reply.started":"2023-11-01T13:18:26.850462Z","shell.execute_reply":"2023-11-01T13:18:38.825035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Retrieval Augmented Generation","metadata":{}},{"cell_type":"markdown","source":"## Comporbar el modelo con HuggingFace pipeline\n\n\nTesteamos el modelo con HF pipeline, usando una query sobre.","metadata":{"execution":{"iopub.status.busy":"2023-09-23T19:22:16.433666Z","iopub.execute_input":"2023-09-23T19:22:16.434937Z","iopub.status.idle":"2023-09-23T19:22:16.440864Z","shell.execute_reply.started":"2023-09-23T19:22:16.434891Z","shell.execute_reply":"2023-09-23T19:22:16.439217Z"}}},{"cell_type":"code","source":"llm = HuggingFacePipeline(pipeline=query_pipeline)\n# checking again that everything is working fine\nllm(prompt=\"Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words.\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-01T13:19:10.730605Z","iopub.execute_input":"2023-11-01T13:19:10.731391Z","iopub.status.idle":"2023-11-01T13:19:16.216107Z","shell.execute_reply.started":"2023-11-01T13:19:10.731356Z","shell.execute_reply":"2023-11-01T13:19:16.215016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"llm = HuggingFacePipeline(pipeline=query_pipeline)\n# checking again that everything is working fine\nllm(prompt=\"Explica que es el discurso sobre el estado de la nación en EE.UU. Hazlo en menos de 100 palabras.\")","metadata":{"execution":{"iopub.status.busy":"2023-11-01T13:20:10.504511Z","iopub.execute_input":"2023-11-01T13:20:10.504920Z","iopub.status.idle":"2023-11-01T13:20:22.457231Z","shell.execute_reply.started":"2023-11-01T13:20:10.504888Z","shell.execute_reply":"2023-11-01T13:20:22.456137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Ingestion usando Text loder\n\nVamos a usar...","metadata":{}},{"cell_type":"code","source":"loader = TextLoader(\"/kaggle/input/tfg-datasets/Listado Preguntas-Respuestas - ONLINE.txt\",\n                    encoding=\"utf8\")\ndocuments = loader.load()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-01T13:32:55.861397Z","iopub.execute_input":"2023-11-01T13:32:55.862418Z","iopub.status.idle":"2023-11-01T13:32:55.873324Z","shell.execute_reply.started":"2023-11-01T13:32:55.862368Z","shell.execute_reply":"2023-11-01T13:32:55.872276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Trocear los datos en chunks\n\nDividimos los datos en chunks utilizando un separador de texto de caracteres recursivo.","metadata":{}},{"cell_type":"code","source":"text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\nall_splits = text_splitter.split_documents(documents)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-01T13:35:03.277804Z","iopub.execute_input":"2023-11-01T13:35:03.278833Z","iopub.status.idle":"2023-11-01T13:35:03.285995Z","shell.execute_reply.started":"2023-11-01T13:35:03.278794Z","shell.execute_reply":"2023-11-01T13:35:03.284743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Crear Embeddings y guardarlos en una BD Vectorial","metadata":{}},{"cell_type":"markdown","source":"Create the embeddings using Sentence Transformer and HuggingFace embeddings.","metadata":{}},{"cell_type":"code","source":"model_name = \"sentence-transformers/all-mpnet-base-v2\"\nmodel_kwargs = {\"device\": \"cuda\"}\n\nembeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-01T13:35:09.500032Z","iopub.execute_input":"2023-11-01T13:35:09.500400Z","iopub.status.idle":"2023-11-01T13:35:16.833843Z","shell.execute_reply.started":"2023-11-01T13:35:09.500371Z","shell.execute_reply":"2023-11-01T13:35:16.832861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Inicializar ChromaDB con los chunks, embeddings y con persistencia local.","metadata":{}},{"cell_type":"code","source":"vectordb = Chroma.from_documents(documents=all_splits, embedding=embeddings, persist_directory=\"chroma_db\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-01T13:35:21.860798Z","iopub.execute_input":"2023-11-01T13:35:21.861202Z","iopub.status.idle":"2023-11-01T13:35:23.374776Z","shell.execute_reply.started":"2023-11-01T13:35:21.861171Z","shell.execute_reply":"2023-11-01T13:35:23.373803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inicializar chain","metadata":{}},{"cell_type":"code","source":"retriever = vectordb.as_retriever()\n\nqa = RetrievalQA.from_chain_type(\n    llm=llm, \n    chain_type=\"stuff\", \n    retriever=retriever, \n    verbose=True\n)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-01T13:35:27.904627Z","iopub.execute_input":"2023-11-01T13:35:27.905053Z","iopub.status.idle":"2023-11-01T13:35:27.911300Z","shell.execute_reply.started":"2023-11-01T13:35:27.905007Z","shell.execute_reply":"2023-11-01T13:35:27.909778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test Retrieval-Augmented Generation \n\n\nDefinimos una función de prueba que ejecutará la consulta y medirá el tiempo.","metadata":{}},{"cell_type":"code","source":"def test_rag(qa, query):\n    print(f\"Query: {query}\\n\")\n    time_1 = time()\n    result = qa.run(query)\n    time_2 = time()\n    print(f\"Inference time: {round(time_2-time_1, 3)} sec.\")\n    print(\"\\nResult: \", result)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-01T13:35:34.862157Z","iopub.execute_input":"2023-11-01T13:35:34.862522Z","iopub.status.idle":"2023-11-01T13:35:34.868148Z","shell.execute_reply.started":"2023-11-01T13:35:34.862494Z","shell.execute_reply":"2023-11-01T13:35:34.867084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Comprobemos algunas consultas.","metadata":{}},{"cell_type":"code","source":"query = \"¿Se pueden adjuntar videos en el depósito del TFG??\"\ntest_rag(qa, query)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-01T13:36:41.543924Z","iopub.execute_input":"2023-11-01T13:36:41.544686Z","iopub.status.idle":"2023-11-01T13:36:53.714157Z","shell.execute_reply.started":"2023-11-01T13:36:41.544649Z","shell.execute_reply":"2023-11-01T13:36:53.713022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"query = \"¿Puedo cambiar de tutor o tema?\"\ntest_rag(qa, query)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-01T13:39:55.221049Z","iopub.execute_input":"2023-11-01T13:39:55.221456Z","iopub.status.idle":"2023-11-01T13:40:07.173565Z","shell.execute_reply.started":"2023-11-01T13:39:55.221427Z","shell.execute_reply":"2023-11-01T13:40:07.172414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fuentes del documento\n\nVerifiquemos las fuentes de documentos para la última consulta ejecutada.","metadata":{}},{"cell_type":"code","source":"docs = vectordb.similarity_search(query)\nprint(f\"Query: {query}\")\nprint(f\"Retrieved documents: {len(docs)}\")\nfor doc in docs:\n    doc_details = doc.to_json()['kwargs']\n    print(\"Source: \", doc_details['metadata']['source'])\n    print(\"Text: \", doc_details['page_content'], \"\\n\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusión\n\nUtilizamos Langchain, ChromaDB y Llama 2 como un Large Language Model (LLM) para construir una solución con Retrieval-Augmented Generation. Para las pruebas, estábamos utilizando...\n\n\n# Mas tranajos relacionados \n\n* https://www.kaggle.com/code/gpreda/test-llama-2-quantized-with-llama-cpp (quantizing LLama 2 model using llama.cpp)\n* https://www.kaggle.com/code/gpreda/fast-test-of-llama-v2-pre-quantized-with-llama-cpp  (quantized Llamam 2 model using llama.cpp)  \n* https://www.kaggle.com/code/gpreda/test-of-llama-2-quantized-with-llama-cpp-on-cpu (quantized model using llama.cpp - running on CPU)\n","metadata":{}},{"cell_type":"markdown","source":"# References  \n\n[1] Murtuza Kazmi, Using LLaMA 2.0, FAISS and LangChain for Question-Answering on Your Own Data, https://medium.com/@murtuza753/using-llama-2-0-faiss-and-langchain-for-question-answering-on-your-own-data-682241488476  \n\n[2] Patrick Lewis, Ethan Perez, et. al., Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, https://browse.arxiv.org/pdf/2005.11401.pdf \n\n[3] Minhajul Hoque, Retrieval Augmented Generation: Grounding AI Responses in Factual Data, https://medium.com/@minh.hoque/retrieval-augmented-generation-grounding-ai-responses-in-factual-data-b7855c059322  \n\n[4] Fangrui Liu\t, Discover the Performance Gain with Retrieval Augmented Generation, https://thenewstack.io/discover-the-performance-gain-with-retrieval-augmented-generation/\n\n[5] Andrew, How to use Retrieval-Augmented Generation (RAG) with Llama 2, https://agi-sphere.com/retrieval-augmented-generation-llama2/   \n\n[6] Yogendra Sisodia, Retrieval Augmented Generation Using Llama2 And Falcon, https://medium.com/@scholarly360/retrieval-augmented-generation-using-llama2-and-falcon-ed26c7b14670","metadata":{}}]}