\capitulo{4}{Técnicas y herramientas}

En esta sección de la memoria, se presentan las técnicas y las herramientas de desarrollo que han sido empleadas en la ejecución de este proyecto. La elección de la metodología y las herramientas adecuadas desempeña un papel fundamental en el éxito y eficiencia de cualquier proyecto. En consecuencia, se llevará a cabo un análisis de diferentes alternativas, considerando aspectos clave como la metodología de desarrollo, las herramientas de implementación y las bibliotecas utilizadas.

\section{\textit{Conversational Retrieval}}

\section{\textit{Querry Transformation}}

\section{\textit{Quantization}}

GGML es una biblioteca Tensor para \textit{machine learning} que se presenta como una biblioteca en C++. Su función principal es permitir la ejecución de modelos de \acrlong{llm} en la CPU o en combinación con la GPU. Un aspecto distintivo de GGML es su definición de un formato binario para la distribución de \acrshort{llm}. Además, GGML emplea una técnica llamada \textit{Quantization} que posibilita la ejecución de modelos de \acrshort{llm} en hardware de consumo\cite{GGML_Gimmi}.

Los pesos de los \acrshort{llm} son números de punto flotante (decimales). Al igual que se necesita más espacio para representar un número entero grande (por ejemplo, 1000) en comparación con un entero pequeño (por ejemplo, 1), se requiere más espacio para representar un número de punto flotante de alta precisión (por ejemplo, 0.0001) en comparación con un número de punto flotante de baja precisión (por ejemplo, 0.1). El proceso de \textit{Quantization} de un modelo de lenguaje grande implica reducir la precisión con la que se representan los pesos para disminuir los recursos necesarios para utilizar el modelo. GGML admite varias estrategias de \textit{Quantization} (por ejemplo, \textit{Quantization} de 4 bits, 5 bits y 8 bits), cada una de las cuales ofrece diferentes compromisos entre eficiencia y rendimiento.

Para utilizar eficazmente los modelos, es esencial tener en cuenta los requisitos de memoria y disco. Dado que los modelos se cargan completamente en la memoria, se necesita suficiente espacio en disco para almacenarlos y RAM suficiente para cargarlos durante la ejecución. En el caso del modelo de 65 mil millones de parámetros, incluso después de la \textit{Quantization}, se recomienda tener al menos 40 gigabytes de RAM disponible. Cabe destacar que los requisitos de memoria y disco son actualmente equivalentes.

\imagen{QuantizedSizeLlama}{Efecto de la \textit{Quantization} en el tamaño de los LLM de LLaMa.}{1}

La \textit{Quantization} desempeña un papel crucial en el manejo de estas demandas de recursos. A menos que se disponga de recursos computacionales excepcionales, la \textit{Quantization} permite utilizar los modelos en configuraciones de hardware más modestas al reducir la precisión de los parámetros del modelo y optimizar el uso de la memoria. Esto garantiza que la ejecución de los modelos siga siendo factible y eficiente para una gama más amplia de configuraciones.

\section{Kaggle}

Kaggle es una plataforma en línea que aloja competiciones de ciencia de datos. Fundada en 2010, Kaggle proporciona un entorno donde científicos de datos y profesionales del aprendizaje automático pueden encontrar conjuntos de datos, participar en competiciones, colaborar en proyectos y mejorar sus habilidades en análisis de datos y modelado predictivo.

Algunas características clave de Kaggle incluyen:
\begin{enumerate}
\item \textbf{Competiciones de Ciencia de Datos:} Kaggle organiza competiciones regulares en las que los participantes compiten para resolver problemas de ciencia de datos planteados por empresas o instituciones. Estos desafíos abarcan una amplia gama de temas, desde reconocimiento de imágenes hasta predicción de precios.

\item \textbf{Conjuntos de Datos Públicos:} Kaggle proporciona un repositorio de conjuntos de datos públicos que los usuarios pueden explorar y utilizar para prácticas y proyectos. Estos conjuntos de datos abarcan diversas áreas, desde datos económicos hasta imágenes médicas.

\item \textbf{Kernels:} Los Kernels son entornos de desarrollo en línea que permiten a los usuarios escribir, ejecutar y compartir código en lenguajes como Python y R. Los Kernels son útiles para la exploración de datos y la creación de modelos.

\item \textbf{Foros y Comunidad:} Kaggle cuenta con una comunidad activa de científicos de datos y profesionales del aprendizaje automático. Los foros permiten la discusión de problemas, la obtención de asesoramiento y el intercambio de conocimientos.

\item \textbf{Aprendizaje y Recursos:} Kaggle ofrece tutoriales, cursos y recursos educativos para ayudar a los usuarios a mejorar sus habilidades en ciencia de datos y aprendizaje automático.
\end{enumerate}

En general, Kaggle ha crecido hasta convertirse en una de las plataformas más importantes para la comunidad de ciencia de datos y el uso d modelos, proporcionando oportunidades para la colaboración, la competencia y el aprendizaje continuo.

\section{LangChain}


\section{Hugging Face}

Hugging Face es una empresa y plataforma que se especializa en modelos de lenguaje natural y \acrlong{dnn}. Ofrecen una amplia gama de recursos y herramientas destinados a facilitar el desarrollo, entrenamiento y despliegue de modelos de \acrfull{pln}).

Algunos aspectos destacados de Hugging Face incluyen\cite{HuggingFace}:
\begin{enumerate}

\item \textbf{Modelos preentrenados:} Hugging Face proporciona acceso a una variedad de modelos de lenguaje natural preentrenados de última generación. Esto incluye modelos como BERT, \acrshort{gpt}, Mistral, LLaMa, RoBERTa, y muchos otros, que han demostrado un rendimiento excepcional en diversas tareas de procesamiento del lenguaje natural.

\item \textbf{Transformers Library:} La Transformers Library de Hugging Face es una biblioteca de código abierto que facilita el uso, entrenamiento y ajuste fino de modelos de transformer para tareas específicas. Proporciona una interfaz consistente para varios modelos y se utiliza ampliamente en la comunidad de aprendizaje profundo para \acrshort{pln}.

\item \textbf{Hugging Face Hub:} Es una plataforma en línea que permite a los desarrolladores compartir, explorar y utilizar modelos de lenguaje natural de Hugging Face. Facilita la colaboración y el intercambio de modelos entrenados por la comunidad.

\item \textbf{Pipeline API:} Hugging Face ofrece una \acrshort{api} de canalización (Pipeline API) que simplifica el uso de modelos complejos para tareas específicas. Esto hace que sea fácil utilizar modelos de \acrshort{pln} preentrenados para clasificación de texto, traducción, resumen y más.

\item \textbf{Comunidad de usuarios:} La plataforma fomenta la colaboración y la contribución de la comunidad al código y los modelos. Los usuarios pueden contribuir con modelos, compartir implementaciones y participar en discusiones relacionadas con el procesamiento del lenguaje natural y la \acrlong{ia}.
\end{enumerate}

En resumen, Hugging Face se ha convertido en un recurso integral para la comunidad de aprendizaje profundo y \acrshort{pln}, proporcionando modelos avanzados, bibliotecas de código abierto y una plataforma para compartir y colaborar en proyectos relacionados con el \acrlong{pln}.

\section{FastAPI}

FastAPI es un \textit{framework} para el desarrollo rápido de \acrshort{api} con Python 3.7 (o superior). Diseñado para ser fácil de usar, rápido y eficiente, FastAPI destaca por su sintaxis declarativa, generación automática de documentación interactiva (compatible con Swagger y ReDoc), y la capacidad de aprovechar al máximo las características de la programación asíncrona. Es una eleccióin muy popular para el desarrollo de RESTful APIs y aplicaciones web.

Una de las principales ventajas de las REST API radica en que el protocolo REST separa el almacenamiento de datos (back-end) y la interfaz de usuario (front-end) del servidor, lo que permite que el cliente y el servidor sean independientes entre sí. Esta separación es fundamental para la arquitectura REST y aporta beneficios significativos al desarrollo de aplicaciones.

La independencia entre el back-end y el front-end facilita la escalabilidad y la flexibilidad del sistema. Al dividir las responsabilidades de manera clara, los equipos de desarrollo pueden trabajar de manera más eficiente y concurrente en las distintas capas de la aplicación. Por ejemplo, el equipo encargado del desarrollo del back-end puede realizar mejoras o modificaciones en la lógica de negocio y en la gestión de datos sin afectar directamente a la interfaz de usuario.

Esta separación también favorece la reutilización de componentes y la portabilidad del software. Dado que el back-end y el front-end operan de manera independiente, es posible implementar cambios o actualizaciones en una capa sin afectar a la otra, siempre y cuando se respeten los contratos definidos por la REST API. Esto simplifica el mantenimiento y permite la integración de nuevas funcionalidades sin perturbar el funcionamiento existente.

Las características clave de FastAPI incluyen:

\begin{itemize}

\item Rápido y Eficiente: Aprovecha las características de Python 3.7+ para proporcionar un rendimiento excepcional.

\item Sintaxis Declarativa: Utiliza anotaciones de tipo estándar de Python para definir los tipos de datos de entrada y salida de las funciones, lo que facilita la validación y la generación automática de documentación.

\item Automatización de Documentación: FastAPI genera automáticamente documentación interactiva basada en las anotaciones de tipo, lo que facilita a los desarrolladores comprender y probar rápidamente la \acrshort{api}.

\item Compatibilidad con Estándares Abiertos: Es compatible con estándares abiertos como OpenAPI (usando Swagger UI y ReDoc) y JSON Schema.

\item Programación Asíncrona: Aprovecha las características de la programación asíncrona para manejar de manera eficiente un gran número de conexiones concurrentes.

\item Seguridad Integrada: Ofrece funciones integradas para manejar la autenticación, autorización y seguridad en general.

\item Interoperabilidad: Puede integrarse fácilmente con otros marcos y bibliotecas de Python.

\end{itemize}

En resumen, FastAPI es una opción robusta y moderna para el desarrollo de \acrshort{api} en Python, destacándose por su enfoque rápido, sintaxis clara y generación automática de documentación.

\section{Streamlit}

Streamlit es una biblioteca de código abierto en Python que se utiliza para crear aplicaciones web interactivas para \textit{data science} y \textit{machine learning} de manera rápida y sencilla. Su objetivo principal es permitir a los usuarios transformar datos en aplicaciones web interactivas con tan solo unas pocas líneas de código.

Con Streamlit, los especialistas de datos y desarrolladores pueden crear fácilmente interfaces de usuario atractivas para sus modelos, visualizaciones y análisis de datos sin necesidad de conocimientos extensos en desarrollo web. La biblioteca se integra bien con bibliotecas populares de Python como Pandas, Matplotlib y Plotly, lo que facilita la creación de aplicaciones web a partir de código existente\cite{Streamlit}.

Streamlit simplifica el proceso de desarrollo de aplicaciones web al manejar automáticamente la actualización de la interfaz de usuario en respuesta a los cambios en los datos subyacentes. Esto permite a los usuarios centrarse más en el análisis de datos y la creación de visualizaciones, sin tener que preocuparse demasiado por los detalles de implementación de la interfaz web.

En resumen, Streamlit es una herramienta valiosa para aquellos que desean hacer una interface para análisis de datos y modelos de aprendizaje automático de manera efectiva a través de aplicaciones web interactivas con una curva de aprendizaje mínima.