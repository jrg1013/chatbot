\capitulo{3}{Conceptos teóricos}

Este apartado de conceptos teóricos desempeña un papel importante al condensar los fundamentos esenciales de principios, teorías y términos subyacentes en el dominio de conocimiento relacionado con el proyecto. En este contexto, su propósito principal es brindar una visión panorámica de los conceptos teóricos cruciales que servirán como cimiento para la comprensión y avance de este \acrlong{tfg}. A través de una exposición minuciosa de estos conceptos, se proporciona una base conceptual que permite una comprensión más profunda de su aplicación práctica en el proyecto. Asimismo, se busca definir una serie de términos y establecer una base de conocimiento compartida para las secciones subsiguientes.

En este contexto, este apartado tiene como objetivo ofrecer una visión general de los conceptos teóricos esenciales, con un énfasis particular en los \acrfull{llm} y las técnicas de \acrfull{rag}. Estos conceptos, son muy actuales y están en constante evolución en el ámbito del procesamiento de lenguaje natural y la inteligencia artificial, desempeñan un papel fundamental en la comprensión y avance del proyecto en consideración.

Los \acrshort{llm} representan un hito significativo en el campo de la generación de texto y la comprensión del lenguaje. Modelos como GPT-3 han demostrado una capacidad sin precedentes para comprender y generar texto de manera coherente y relevante, convirtiéndose en herramientas poderosas con diversas aplicaciones.

Las técnicas de \acrshort{rag} amplían aún más el potencial de los \acrshort{llm} al permitir el acceso a información específica en documentos o bases de conocimiento existentes. Esta capacidad de recuperación y generación mejorada se traduce en respuestas más precisas y contextualmente relevantes, lo que resulta particularmente valioso en situaciones que requieren asistencia y generación de contenido.

En el núcleo de los \acrshort{llm} y las técnicas de \acrshort{rag}, se encuentran otros conceptos fundamentales de embeddings y bases de datos vectoriales. Estos conceptos son la columna vertebral que impulsa la capacidad de estos modelos para entender y generar texto de manera efectiva. Los embeddings, representaciones vectoriales de palabras y frases, permiten a los \acrshort{llm} comprender y procesar el lenguaje natural, capturando la semántica y relaciones entre palabras. Por otro lado, las bases de datos vectoriales almacenan información en un espacio vectorial, lo que habilita la recuperación eficiente de información relevante. La interacción sinérgica de estos conceptos permite a los \acrshort{llm} y las técnicas \acrshort{rag} acceder a conocimiento específico y generar respuestas contextualmente enriquecidas, mejorando la precisión y relevancia en la comunicación y generación de contenido.

\section{¿Que es un ``\textit{Large Language Models}''?}

En esencia, un modelo lingüístico de gran escala es un tipo de modelo de aprendizaje automático que puede comprender y generar lenguaje humano mediante redes neuronales profundas (\acrlong{dnn}). La principal tarea de un modelo lingüístico es calcular la probabilidad de que una palabra siga a una entrada dada en una frase: por ejemplo, ``El cielo es ....'', siendo la respuesta más probable ``azul''. El modelo es capaz de predecir la siguiente palabra de una frase tras recibir un amplio conjunto de datos de texto (o corpus). Básicamente, aprende a reconocer distintos patrones en las palabras. De este proceso se obtiene un modelo lingüístico preentrenado.

Si se ajustan un poco, estos modelos pueden tener diversos usos prácticos, como la traducción o la adquisición de conocimientos especializados en un campo concreto, como el Derecho o la Medicina. Este proceso se conoce como aprendizaje por transferencia, que permite a un modelo aplicar los conocimientos adquiridos de una tarea a otra.

Lo que hace que un modelo lingüístico sea ``grande'' es el tamaño de su arquitectura. Ésta, a su vez, se basa en la inteligencia artificial de las redes neuronales, muy parecidas al cerebro humano, donde las neuronas trabajan juntas para aprender de la información y procesarla. Además, los \acrshort{llm} constan de un gran número de parámetros (por ejemplo, \acrshort{gpt} tiene más de 100.000 millones) entrenados en grandes cantidades de datos de texto sin etiquetar mediante aprendizaje autosupervisado o semisupervisado. Con el primero, los modelos son capaces de aprender a partir de texto no anotado, lo que supone una gran ventaja si se tienen en cuenta los costosos inconvenientes de tener que depender de datos etiquetados manualmente.

Además, las redes más grandes y con más parámetros han demostrado un mejor rendimiento, con una mayor capacidad para retener información y reconocer patrones en comparación con sus homólogas más pequeñas. Cuanto mayor es el modelo, más información puede aprender durante el proceso de entrenamiento, lo que a su vez hace que sus predicciones sean más precisas. Aunque esto puede ser cierto en el sentido convencional, hay una salvedad: tanto las empresas de \acrshort{ia} como los desarrolladores están encontrando formas de sortear los retos que plantean los excesivos costes computacionales y la energía necesaria para entrenar los \acrshort{llm} introduciendo modelos más pequeños y entrenados de forma más óptima.

Aunque los \acrshort{llm}  se han entrenado principalmente para tareas sencillas, como predecir la siguiente palabra de una frase, es asombroso ver la cantidad de estructura y significado del lenguaje que han sido capaces de captar, por no mencionar el enorme número de datos que pueden recoger.

\subsection{Historia y desarrollo de los LLM}

La historia y evolución de los \acrshort{llm} se remontan a varias décadas de investigación y desarrollo en el campo del procesamiento de lenguaje natural y la \acrlong{ia}. A continuación, se proporciona un resumen de los hitos más significativos en la historia de los \acrshort{llm}\cite{zhao2023survey,scribbleData,Tolaka}.

\imagen{LLL_Evolution}{Cronología de la evolución de los LLM}{1}

\begin{description}

\item[Década de 1950-1960]: Los primeros pasos en la creación de modelos de lenguaje se remonta a los experimentos pioneros con redes neuronales y sistemas de procesamiento de información neuronal realizados en la década de 1950 con el propósito de permitir a las computadoras comprender y procesar el lenguaje natural. Colaboraciones entre investigadores de IBM y la Universidad de Georgetown dieron lugar a la creación de un sistema capaz de traducir automáticamente frases del ruso al inglés, lo que marcó un hito notable en la traducción automática. A partir de ese punto, la investigación en este campo experimentó un auge significativo. Durante esta misma época, se dieron los primeros pasos en el desarrollo de modelos de lenguaje, con investigadores dedicados a la creación de reglas gramaticales y algoritmos para analizar y generar texto. Sin embargo, estos enfoques iniciales se basaban en reglas manuales y presentaban limitaciones significativas en cuanto a su efectividad.

La idea de los \acrshort{llm} surgió con la creación de \textit{Eliza} en los años 60, fue el primer chatbot del mundo, diseñado por el investigador del \acrshort{mit} Joseph Weizenbaum. \textit{Eliza} marcó el inicio de la investigación sobre el \acrfull{pln} y sentó las bases para futuros \acrshort{llm} más complejos.

\item[Década de 1980-1990]: Se produjeron avances en el \acrfull{pln} con la introducción de modelos estadísticos y técnicas de aprendizaje automático. Modelos como el modelo de lenguaje de Markov oculto (HMM) se convirtieron en populares para tareas de \acrshort{pln}. Una de las innovaciones mas significativas fue la introducción de las redes de memoria a largo plazo (LSTM) en 1997, que permitieron crear redes neuronales más profundas y complejas, capaces de manejar cantidades de datos más significativas.

\item[Década de 2000-2010]: Otro momento crucial fue la suite CoreNLP de Stanford, introducida en 2010. Esta suite ofrecía un conjunto de herramientas y algoritmos que ayudaban a los investigadores a abordar tareas de \acrshort{pln} complejas, como el análisis de sentimientos y el reconocimiento de entidades con nombre. Surgieron también modelos estadísticos más avanzados, como los modelos de lenguaje basados en \acrfull{svm} y las \acrfull{crf}. Estos modelos mejoraron la capacidad de procesar y generar texto de manera más efectiva.

\item[Década de 2010-2020]: Esta década marcó un hito significativo con la llegada de modelos basados en redes neuronales, especialmente modelos de lenguaje recurrente (RNN) y modelos de lenguaje basados en \textit{Transformers}. En 2011, Google Brain hizo su debut, proporcionando a los investigadores un acceso invaluable a recursos informáticos de gran potencia y conjuntos de datos enriquecidos, además de ofrecer características avanzadas, como la incrustación de palabras. Esta innovación permitió a los sistemas de \acrlong{pln} comprender el contexto de las palabras de manera más efectiva. 

El trabajo pionero de Google Brain sentó las bases para avances significativos en el campo, incluyendo la aparición de los modelos \textit{Transformer} en 2017. La arquitectura de los \textit{Transformers} revolucionó la creación de \acrfull{llm} más grandes y sofisticados, ejemplificados por el \acrfull{gpt} de OpenAI. Uno de los modelos más influyentes es el GPT-1 desarrollado por OpenAI en 2018.

GPT-2, una versión más grande y avanzada de GPT-1, causó revuelo en la comunidad de inteligencia artificial debido a su capacidad para generar texto coherente y de alta calidad. OpenAI inicialmente decidió no publicar GPT-2 debido a preocupaciones sobre el uso malicioso. 

Fue en 2019 cuando los investigadores de Google presentaron BERT, el modelo bidireccional de 340 millones de parámetros (el tercer modelo más grande de su clase) que podía determinar el contexto permitiéndole adaptarse a diversas tareas. Al preentrenar a BERT en una amplia variedad de datos no estructurados mediante aprendizaje autosupervisado, el modelo pudo comprender las relaciones entre las palabras. En poco tiempo, BERT se convirtió en la herramienta de referencia para las tareas de procesamiento del lenguaje natural. De hecho, BERT estaba detrás de todas las consultas en inglés realizadas a través de Google Search.

\item[2020 en adelante]: GPT-3, lanzado por OpenAI, marcó un avance significativo en el campo de los \acrshort{llm}. Con 175 mil millones de parámetros, GPT-3 demostró una sorprendente capacidad para comprender y generar texto de manera coherente. Se convirtió en un modelo base para muchas aplicaciones de procesamiento de lenguaje natural y impulso el movimiento basado ``\textit{Generative AI}'' al gran público, especialmente a través de la \textit{interface} Chat-GPT.

Desde GPT-3, la investigación en \acrshort{llm} ha continuado avanzando. Se han desarrollado modelos aún más grandes y efectivos, y se han aplicado a una amplia gama de aplicaciones, desde asistentes virtuales hasta traducción automática y generación creativa de texto o imagen.

La versión más reciente hasta la fecha es GPT-4, que presenta mejoras significativas, como la capacidad de utilizar visión por ordenador para interpretar datos visuales (a diferencia de ChatGPT, que utiliza GPT-3.5). GPT-4 acepta como entrada tanto texto, como imágenes. 

Y lo que es más, el último avance es la \textit{steerability} (direccionabilidad), que permite a los usuarios de \acrshort{gpt} personalizar la estructura de su salida para satisfacer sus necesidades específicas. Básicamente, la direccionabilidad alude a la capacidad de controlar o modificar el comportamiento de un modelo lingüístico, lo que implica hacer que el \acrshort{llm} adopte distintos roles, siga instrucciones del usuario o hable con un tono determinado. La direccionabilidad permite al usuario cambiar el comportamiento de un \acrshort{llm} a voluntad y ordenarle que escriba con un estilo o una voz diferentes. Las posibilidades son infinitas.

\end{description}

\imagen{Timeline_LLM2}{Cronología de los modelos lingüísticos de gran tamaño (superior a 10B) de los últimos años}{1}

Es importante destacar que la evolución de los \acrshort{llm} ha sido impulsada en gran medida por el aumento en la disponibilidad de datos de entrenamiento, el desarrollo de arquitecturas de redes neuronales más avanzadas y la mejora en el hardware de cómputo. Estos avances han permitido a los \acrshort{llm} alcanzar un nivel de comprensión y generación de texto que antes era impensable.

\section{Como funciona un LLM}



\subsection{Word2Vec}

Word2Vec fue un algoritmo revolucionario en el \acrfull{pln} que se emplea para aprender representaciones vectoriales densas de palabras a partir de grandes cantidades de texto. Desarrollado por un equipo de investigadores de Google en 2013\cite{Mikolov2013Word2Vec}, este enfoque no supervisado ha tenido un impacto significativo en el campo del \acrshort{pln}. Construyeron un modelo para incrustar palabras en un espacio vectorial, un problema que ya contaba con una larga historia académica en aquel momento, que comenzaba en la década de 1980. Su modelo utilizaba un objetivo de optimización diseñado para convertir las relaciones de correlación entre palabras en relaciones de distancia en el espacio de incrustación: se asociaba un vector a cada palabra de un vocabulario, y los vectores se optimizaban para que el producto punto (proximidad del coseno) entre vectores que representaban palabras que coincidían con frecuencia estuviera más cerca de 1, mientras que el producto punto entre vectores que representaban palabras que rara vez coincidían estuviera más cerca de 0. Descubrieron que el espacio de incrustación resultante era un espacio vectorial. 

El espacio de incrustación resultante hacía mucho más que captar la similitud semántica. Presentaba alguna forma de aprendizaje emergente: era capaz de realizar ``aritmética de palabras'', algo para lo que no había sido entrenado. Existía un vector en el espacio que podía añadirse a cualquier sustantivo masculino para obtener un punto cercano a su equivalente femenino. Por ejemplo, V(rey) - V(hombre) + V(mujer) = V(reina). Un ``vector de género". Esta capacidad de realizar operaciones matemáticas en el espacio de incrustación reveló una comprensión subyacente de las relaciones semánticas. Parecía haber docenas de vectores mágicos de este tipo: un vector plural, un vector para pasar de nombres de animales salvajes a su equivalente más cercano en mascotas, y muchos otros. Estos descubrimientos abrieron nuevas perspectivas en el campo del procesamiento de lenguaje natural y subrayaron la potencia de Word2Vec en la representación de palabras\cite{Chollet}.

\imagen{Word2Vec}{Ilustración de un espacio de incrustación 2D tal que el vector que une ``lobo'' con ``perro'' es el mismo que el vector que une ``tigre'' con ``gato''.}{0.5}

Word2Vec se implementa en dos arquitecturas principales: \acrfull{cbow} y \textit{Skip-gram}. El modelo \acrshort{cbow} se utiliza para predecir una palabra objetivo basada en un contexto circundante, mientras que el modelo \textit{Skip-gram} realiza predicciones inversas, es decir, predice palabras de contexto a partir de una palabra dada. Estas arquitecturas han demostrado ser altamente efectivas en la generación de representaciones vectoriales de palabras que capturan significados y relaciones con precisión.

Las representaciones de palabras aprendidas con Word2Vec se han convertido en una pieza fundamental en tareas de transferencia de conocimiento en \acrshoert
pln, lo que ha impulsado su amplia adopción en la comunidad de investigación y desarrollo. A través de su capacidad para reducir la dimensionalidad y su habilidad para mejorar el rendimiento en tareas de procesamiento de lenguaje, Word2Vec ha dejado una huella perdurable en la forma en que las computadoras comprenden y utilizan el lenguaje natural en una variedad de aplicaciones.

\section{Tipos de LLM}

\imagen{TiposLLM}{Clasificación de los LLM en tres categorias}{1}

\section{¿Que es un \textit{Retrieval-augmented Generation}?}

Los LLM han demostrado su capacidad para comprender el contexto y ofrecer respuestas precisas a diversas tareas de procesamiento de lenguaje natural (PLN), como la síntesis o las preguntas y respuestas, cuando se les solicita. Aunque son capaces de ofrecer muy buenas respuestas a preguntas sobre información con la que fueron entrenados, tienden a ``alucinar'' cuando el tema trata sobre información que desconocen, es decir, que no estaba incluida en sus datos de entrenamiento.

``\textit{Retrieval-augmented Generation}'' (RAG) es una técnica avanzada en el campo del PNL que combina dos enfoques clave: recuperación y generación de texto. Esta técnica se utiliza para mejorar la generación de texto automática y garantizar que las respuestas generadas sean precisas, relevantes y contextualmente adecuadas\cite{Lewis2020}.

En un sistema de RAG, el proceso se divide en dos etapas:

\begin{enumerate}
    \item Recuperación (\textit{Retrieval}): En esta etapa, el sistema busca información relevante en grandes conjuntos de datos o bases de conocimiento. Utiliza métodos de recuperación de información para encontrar documentos o fragmentos de texto que contienen información relacionada con la consulta o el contexto actual.

    \item Generación (\textit{Generation}): Una vez que se ha recuperado la información relevante, el sistema de generación de texto (a menudo basado en un LLM, como GPT) utiliza esta información para generar respuestas coherentes y contextualmente apropiadas.
    
\end{enumerate}

La combinación de estas dos etapas permite que la técnica RAG proporcione respuestas que no solo se basen en el conocimiento preexistente\cite{chen-etal-2017-reading}, sino que también sean sensibles al contexto específico de la consulta o la tarea. Esto propicia respuestas más precisas y relevantes en comparación con enfoques puramente generativos\cite{fan-etal-2019-eli5,hossain-etal-2020-simple}.

RAG se utiliza en una variedad de aplicaciones, incluyendo chatbots, sistemas de respuesta automática, motores de búsqueda mejorados y generación de contenido automático, donde la capacidad de acceder y utilizar información específica es esencial para brindar respuestas mas precisas.

\section{Embeddings}

\section{Bases de datos Vectoriales}

\section{Selección de un LLM}

Aspectos a considerar: Precio, tamaño, funciones.

\subsection{OpenAI}

\imagen{OpenAI}{Evolución tecnológica de los modelos GPT}{1}

\subsection{LLaMa}

\cite{Murtuza}

\imagen{LLaMa}{Gráfico de la evolución de los modelos LLaMa}{1}

\subsection{Bard}

\subsection{Mistral}

\cite{Zhong2023AGIEvalAH}
\imagen{Mistral_Comparacion}{Resultados en MMLU, Razonamiento de sentido común, Conocimiento del mundo y Comprensión lectora para Mistral 7B y Llama 2 (7B/13/70B).}{1}

\subsection{Otros LLM}

MiniLLMs

\section{Prompt Engineering}

\section{Tablas}

Igualmente se pueden usar los comandos específicos de \LaTeX o bien usar alguno de los comandos de la plantilla.

\tablaSmall{Herramientas y tecnologías utilizadas en cada parte del proyecto}{l c c c c}{herramientasportipodeuso}
{ \multicolumn{1}{l}{Herramientas} & App AngularJS & API REST & BD & Memoria \\}{ 
HTML5 & X & & &\\
CSS3 & X & & &\\
BOOTSTRAP & X & & &\\
JavaScript & X & & &\\
AngularJS & X & & &\\
Bower & X & & &\\
PHP & & X & &\\
Karma + Jasmine & X & & &\\
Slim framework & & X & &\\
Idiorm & & X & &\\
Composer & & X & &\\
JSON & X & X & &\\
PhpStorm & X & X & &\\
MySQL & & & X &\\
PhpMyAdmin & & & X &\\
Git + BitBucket & X & X & X & X\\
Mik\TeX{} & & & & X\\
\TeX{}Maker & & & & X\\
Astah & & & & X\\
Balsamiq Mockups & X & & &\\
VersionOne & X & X & X & X\\
} 
