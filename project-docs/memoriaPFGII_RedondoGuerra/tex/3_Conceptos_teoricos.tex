\capitulo{3}{Conceptos teóricos}

Este apartado de conceptos teóricos desempeña un papel importante al condensar los fundamentos esenciales de principios, teorías y términos subyacentes en el dominio de conocimiento relacionado con el proyecto. En este contexto, su propósito principal es brindar una visión panorámica de los conceptos teóricos cruciales que servirán como cimiento para la comprensión y avance de este \acrlong{tfg}. A través de una exposición minuciosa de estos conceptos, se proporciona una base conceptual que permite una comprensión más profunda de su aplicación práctica en el proyecto. Asimismo, se busca definir una serie de términos y establecer una base de conocimiento compartida para las secciones subsiguientes.

En este contexto, este apartado tiene como objetivo ofrecer una visión general de los conceptos teóricos esenciales, con un énfasis particular en los \acrfull{llm} y las técnicas de \acrfull{rag}. Estos conceptos, son muy actuales y están en constante evolución en el ámbito del procesamiento de lenguaje natural y la inteligencia artificial, desempeñan un papel fundamental en la comprensión y avance del proyecto en consideración.

Los \acrshort{llm} representan un hito significativo en el campo de la generación de texto y la comprensión del lenguaje. Modelos como GPT-3 han demostrado una capacidad sin precedentes para comprender y generar texto de manera coherente y relevante, convirtiéndose en herramientas poderosas con diversas aplicaciones.

Las técnicas de \acrshort{rag} amplían aún más el potencial de los \acrshort{llm} al permitir el acceso a información específica en documentos o bases de conocimiento existentes. Esta capacidad de recuperación y generación mejorada se traduce en respuestas más precisas y contextualmente relevantes, lo que resulta particularmente valioso en situaciones que requieren asistencia y generación de contenido.

En el núcleo de los \acrshort{llm} y las técnicas de \acrshort{rag}, se encuentran otros conceptos fundamentales de embeddings y bases de datos vectoriales. Estos conceptos son la columna vertebral que impulsa la capacidad de estos modelos para entender y generar texto de manera efectiva. Los embeddings, representaciones vectoriales de palabras y frases, permiten a los \acrshort{llm} comprender y procesar el lenguaje natural, capturando la semántica y relaciones entre palabras. Por otro lado, las bases de datos vectoriales almacenan información en un espacio vectorial, lo que habilita la recuperación eficiente de información relevante. La interacción sinérgica de estos conceptos permite a los \acrshort{llm} y las técnicas \acrshort{rag} acceder a conocimiento específico y generar respuestas contextualmente enriquecidas, mejorando la precisión y relevancia en la comunicación y generación de contenido.

\section{¿Que es un ``\textit{Large Language Models}''?}

En esencia, un modelo lingüístico de gran escala es un tipo de modelo de aprendizaje automático que puede comprender y generar lenguaje humano mediante redes neuronales profundas (\acrlong{dnn}). La principal tarea de un modelo lingüístico es calcular la probabilidad de que una palabra siga a una entrada dada en una frase: por ejemplo, ``El cielo es ....'', siendo la respuesta más probable ``azul''. El modelo es capaz de predecir la siguiente palabra de una frase tras recibir un amplio conjunto de datos de texto (o \textit{corpus}). Básicamente, aprende a reconocer distintos patrones en las palabras. De este proceso se obtiene un modelo lingüístico preentrenado.

Si se ajustan un poco, estos modelos pueden tener diversos usos prácticos, como la traducción o la adquisición de conocimientos especializados en un campo concreto, como el Derecho o la Medicina. Este proceso se conoce como aprendizaje por transferencia, que permite a un modelo aplicar los conocimientos adquiridos de una tarea a otra.

Lo que hace que un modelo lingüístico sea ``grande'' es el tamaño de su arquitectura. Ésta, a su vez, se basa en la inteligencia artificial de las redes neuronales, muy parecidas al cerebro humano, donde las neuronas trabajan juntas para aprender de la información y procesarla. Además, los \acrshort{llm} constan de un gran número de parámetros (por ejemplo, \acrshort{gpt} tiene más de 100.000 millones) entrenados en grandes cantidades de datos de texto sin etiquetar mediante aprendizaje autosupervisado o semisupervisado. Con el primero, los modelos son capaces de aprender a partir de texto no anotado, lo que supone una gran ventaja si se tienen en cuenta los costosos inconvenientes de tener que depender de datos etiquetados manualmente.

Además, las redes más grandes y con más parámetros han demostrado un mejor rendimiento, con una mayor capacidad para retener información y reconocer patrones en comparación con sus homólogas más pequeñas. Cuanto mayor es el modelo, más información puede aprender durante el proceso de entrenamiento, lo que a su vez hace que sus predicciones sean más precisas. Aunque esto puede ser cierto en el sentido convencional, hay una salvedad: tanto las empresas de \acrshort{ia} como los desarrolladores están encontrando formas de sortear los retos que plantean los excesivos costes computacionales y la energía necesaria para entrenar los \acrshort{llm} introduciendo modelos más pequeños y entrenados de forma más óptima.

Aunque los \acrshort{llm}  se han entrenado principalmente para tareas sencillas, como predecir la siguiente palabra de una frase, es asombroso ver la cantidad de estructura y significado del lenguaje que han sido capaces de captar, por no mencionar el enorme número de datos que pueden recoger.

\subsection{Historia y desarrollo de los LLM}

La historia y evolución de los \acrshort{llm} se remontan a varias décadas de investigación y desarrollo en el campo del procesamiento de lenguaje natural y la \acrlong{ia}. A continuación, se proporciona un resumen de los hitos más significativos en la historia de los \acrshort{llm}\cite{zhao2023survey,scribbleData,Tolaka}.

\imagen{LLL_Evolution}{Cronología de la evolución de los LLM}{1}

\begin{description}

\item[Década de 1950-1960]: Los primeros pasos en la creación de modelos de lenguaje se remonta a los experimentos pioneros con redes neuronales y sistemas de procesamiento de información neuronal realizados en la década de 1950 con el propósito de permitir a las computadoras comprender y procesar el lenguaje natural. Colaboraciones entre investigadores de IBM y la Universidad de Georgetown dieron lugar a la creación de un sistema capaz de traducir automáticamente frases del ruso al inglés, lo que marcó un hito notable en la traducción automática. A partir de ese punto, la investigación en este campo experimentó un auge significativo. Durante esta misma época, se dieron los primeros pasos en el desarrollo de modelos de lenguaje, con investigadores dedicados a la creación de reglas gramaticales y algoritmos para analizar y generar texto. Sin embargo, estos enfoques iniciales se basaban en reglas manuales y presentaban limitaciones significativas en cuanto a su efectividad.

La idea de los \acrshort{llm} surgió con la creación de \textit{Eliza} en los años 60, fue el primer chatbot del mundo, diseñado por el investigador del \acrshort{mit} Joseph Weizenbaum. \textit{Eliza} marcó el inicio de la investigación sobre el \acrfull{pln} y sentó las bases para futuros \acrshort{llm} más complejos.

\item[Década de 1980-1990]: Se produjeron avances en el \acrfull{pln} con la introducción de modelos estadísticos y técnicas de aprendizaje automático. Modelos como el modelo de lenguaje de Markov oculto (HMM) se convirtieron en populares para tareas de \acrshort{pln}. Una de las innovaciones mas significativas fue la introducción de las redes de memoria a largo plazo (LSTM) en 1997, que permitieron crear redes neuronales más profundas y complejas, capaces de manejar cantidades de datos más significativas.

\item[Década de 2000-2010]: Otro momento crucial fue la suite CoreNLP de Stanford, introducida en 2010. Esta suite ofrecía un conjunto de herramientas y algoritmos que ayudaban a los investigadores a abordar tareas de \acrshort{pln} complejas, como el análisis de sentimientos y el reconocimiento de entidades con nombre. Surgieron también modelos estadísticos más avanzados, como los modelos de lenguaje basados en \acrfull{svm} y las \acrfull{crf}. Estos modelos mejoraron la capacidad de procesar y generar texto de manera más efectiva.

\item[Década de 2010-2020]: Esta década marcó un hito significativo con la llegada de modelos basados en redes neuronales, especialmente modelos de lenguaje recurrente (RNN) y modelos de lenguaje basados en \textit{Transformers}. En 2011, Google Brain hizo su debut, proporcionando a los investigadores un acceso invaluable a recursos informáticos de gran potencia y conjuntos de datos enriquecidos, además de ofrecer características avanzadas, como la incrustación de palabras. Esta innovación permitió a los sistemas de \acrlong{pln} comprender el contexto de las palabras de manera más efectiva. 

El trabajo pionero de Google Brain sentó las bases para avances significativos en el campo, incluyendo la aparición de los modelos \textit{Transformer} en 2017. La arquitectura de los \textit{Transformers} revolucionó la creación de \acrfull{llm} más grandes y sofisticados, ejemplificados por el \acrfull{gpt} de OpenAI. Uno de los modelos más influyentes es el GPT-1 desarrollado por OpenAI en 2018.

GPT-2, una versión más grande y avanzada de GPT-1, causó revuelo en la comunidad de inteligencia artificial debido a su capacidad para generar texto coherente y de alta calidad. OpenAI inicialmente decidió no publicar GPT-2 debido a preocupaciones sobre el uso malicioso. 

Fue en 2019 cuando los investigadores de Google presentaron BERT, el modelo bidireccional de 340 millones de parámetros (el tercer modelo más grande de su clase) que podía determinar el contexto permitiéndole adaptarse a diversas tareas. Al preentrenar a BERT en una amplia variedad de datos no estructurados mediante aprendizaje autosupervisado, el modelo pudo comprender las relaciones entre las palabras. En poco tiempo, BERT se convirtió en la herramienta de referencia para las tareas de procesamiento del lenguaje natural. De hecho, BERT estaba detrás de todas las consultas en inglés realizadas a través de Google Search.

\item[2020 en adelante]: GPT-3, lanzado por OpenAI, marcó un avance significativo en el campo de los \acrshort{llm}. Con 175 mil millones de parámetros, GPT-3 demostró una sorprendente capacidad para comprender y generar texto de manera coherente. Se convirtió en un modelo base para muchas aplicaciones de procesamiento de lenguaje natural y impulso el movimiento basado ``\textit{Generative AI}'' al gran público, especialmente a través de la \textit{interface} Chat-GPT.

Desde GPT-3, la investigación en \acrshort{llm} ha continuado avanzando. Se han desarrollado modelos aún más grandes y efectivos, y se han aplicado a una amplia gama de aplicaciones, desde asistentes virtuales hasta traducción automática y generación creativa de texto o imagen.

La versión más reciente hasta la fecha es GPT-4, que presenta mejoras significativas, como la capacidad de utilizar visión por ordenador para interpretar datos visuales (a diferencia de ChatGPT, que utiliza GPT-3.5). GPT-4 acepta como entrada tanto texto, como imágenes. 

Y lo que es más, el último avance es la \textit{steerability} (direccionabilidad), que permite a los usuarios de \acrshort{gpt} personalizar la estructura de su salida para satisfacer sus necesidades específicas. Básicamente, la direccionabilidad alude a la capacidad de controlar o modificar el comportamiento de un modelo lingüístico, lo que implica hacer que el \acrshort{llm} adopte distintos roles, siga instrucciones del usuario o hable con un tono determinado. La direccionabilidad permite al usuario cambiar el comportamiento de un \acrshort{llm} a voluntad y ordenarle que escriba con un estilo o una voz diferentes. Las posibilidades son infinitas.

\end{description}

\imagen{Timeline_LLM2}{Cronología de los modelos lingüísticos de gran tamaño (superior a 10B) de los últimos años}{1}

Es importante destacar que la evolución de los \acrshort{llm} ha sido impulsada en gran medida por el aumento en la disponibilidad de datos de entrenamiento, el desarrollo de arquitecturas de redes neuronales más avanzadas y la mejora en el hardware de cómputo. Estos avances han permitido a los \acrshort{llm} alcanzar un nivel de comprensión y generación de texto que antes era impensable.

\subsection{Word2Vec}

Un precursor de los actuales \acrlong{llm} que merece la pena destacar es Word2Vec. Word2Vec fue un algoritmo revolucionario en el \acrfull{pln} que se emplea para aprender representaciones vectoriales densas de palabras a partir de grandes cantidades de texto. Desarrollado por un equipo de investigadores de Google en 2013\cite{Mikolov2013Word2Vec}, este enfoque no supervisado ha tenido un impacto significativo en el campo del \acrshort{pln}. Construyeron un modelo para incrustar palabras en un espacio vectorial, un problema que ya contaba con una larga historia académica en aquel momento, que comenzaba en la década de 1980. Su modelo utilizaba un objetivo de optimización diseñado para convertir las relaciones de correlación entre palabras en relaciones de distancia en el espacio de incrustación: se asociaba un vector a cada palabra de un vocabulario, y los vectores se optimizaban para que el producto punto (proximidad del coseno) entre vectores que representaban palabras que coincidían con frecuencia estuviera más cerca de 1, mientras que el producto punto entre vectores que representaban palabras que rara vez coincidían estuviera más cerca de 0. Descubrieron que el espacio de incrustación resultante era un espacio vectorial. 

El espacio de incrustación resultante hacía mucho más que captar la similitud semántica. Presentaba alguna forma de aprendizaje emergente: era capaz de realizar ``aritmética de palabras'', algo para lo que no había sido entrenado. Existía un vector en el espacio que podía añadirse a cualquier sustantivo masculino para obtener un punto cercano a su equivalente femenino. Por ejemplo, V(rey) - V(hombre) + V(mujer) = V(reina). Un ``vector de género". Esta capacidad de realizar operaciones matemáticas en el espacio de incrustación reveló una comprensión subyacente de las relaciones semánticas. Parecía haber docenas de vectores mágicos de este tipo: un vector plural, un vector para pasar de nombres de animales salvajes a su equivalente más cercano en mascotas, y muchos otros. Estos descubrimientos abrieron nuevas perspectivas en el campo del procesamiento de lenguaje natural y subrayaron la potencia de Word2Vec en la representación de palabras\cite{Chollet}.

\imagen{Word2Vec}{Ilustración de un espacio de incrustación 2D tal que el vector que une ``lobo'' con ``perro'' es el mismo que el vector que une ``tigre'' con ``gato''.}{0.5}

Word2Vec se implementa en dos arquitecturas principales: \acrfull{cbow} y \textit{Skip-gram}. El modelo \acrshort{cbow} se utiliza para predecir una palabra objetivo basada en un contexto circundante, mientras que el modelo \textit{Skip-gram} realiza predicciones inversas, es decir, predice palabras de contexto a partir de una palabra dada. Estas arquitecturas han demostrado ser altamente efectivas en la generación de representaciones vectoriales de palabras que capturan significados y relaciones con precisión.

Las representaciones de palabras aprendidas con Word2Vec se han convertido en una pieza fundamental en tareas de transferencia de conocimiento en \acrshoert{pln}, lo que ha impulsado su amplia adopción en la comunidad de investigación y desarrollo. A través de su capacidad para reducir la dimensionalidad y su habilidad para mejorar el rendimiento en tareas de procesamiento de lenguaje, Word2Vec ha dejado una huella perdurable en la forma en que las computadoras comprenden y utilizan el lenguaje natural en una variedad de aplicaciones.



\section{Como funciona un LLM}

Un \acrfull{llm} es un tipo de modelo lingüístico que destaca por su capacidad de comprensión y generación de lenguaje de propósito general. Los \acrshort{llm} adquieren estas capacidades utilizando cantidades masivas de datos, llamados \textit{corpus}, para aprender miles de millones de parámetros durante el entrenamiento y consumiendo grandes recursos computacionales durante su formación y funcionamiento\cite{radford_language_2019}. Los \acrshort{llm} son redes neuronales artificiales,principalmente \textit{transformers}\cite{Nvidia_Transformers}, y se preentrenan utilizando aprendizaje autosupervisado y aprendizaje semisupervisado.

Como modelos autorregresivos del lenguaje, funcionan tomando un texto de entrada y prediciendo repetidamente el siguiente token o palabra\cite{bowman2023eight}. Hasta 2020, el \textit{fine-tuning} era la única forma de adaptar un modelo para que pudiera realizar tareas específicas. Sin embargo, los modelos de mayor tamaño, como el GPT-3, pueden diseñarse con \textit{prompt-engineering} para lograr resultados similares. Se cree que adquieren conocimientos incorporados sobre sintaxis, semántica y "ontología" inherentes a los del lenguaje humano, pero también imprecisiones y sesgos presentes en los en el mismo.

\subsection{\textit{Transformers}}

Los \textit{Transformers} son una arquitectura de \acrfull{dnn} que ha revolucionado el campo del \acrfull{pln} y ha impulsado el desarrollo de los \acrfull{llm}. Estas arquitecturas se destacan por su capacidad para modelar relaciones y dependencias de largo alcance en datos secuenciales, como texto, de manera altamente eficiente\cite{Nvidia_Transformers}.

La clave de los \textit{Transformers} radica en su estructura de atención, que permite que el modelo procese secuencias de entrada de manera paralela y capture relaciones entre palabras en diferentes posiciones de la secuencia\cite{Techtarget_LLM}. A continuación, se describe cómo funcionan los \textit{Transformers} aplicados a los \acrshort{LLM}:

\begin{enumerate}
    
\item \textbf{Codificación de entrada}: La secuencia de entrada (por ejemplo, una oración o un párrafo) se descompone en una serie de vectores de palabras o tokens. Cada palabra se representa como un vector de números reales.

\item \textbf{Capas de atención}: El corazón de la arquitectura de los \textit{Transformer} es la capa de atención. Esta capa calcula la atención entre todas las palabras en la secuencia de entrada. La atención es una medida de cuánta importancia se le asigna a cada palabra en función de su relación con otras palabras en la secuencia. Esta atención se calcula a través de una función de similitud, que pondera las conexiones entre las palabras.

\item \textbf{Codificación posicional}: Aunque las capas de atención capturan relaciones entre palabras, los \textit{Transformers} también necesitan información sobre la posición de las palabras en la secuencia. Para lograr esto, se agrega información de codificación posicional a los vectores de palabras.

\item \textbf{Apilamiento de capas}: Los \textit{Transformers} constan de múltiples capas de atención. Cada capa refina las representaciones de las palabras y las relaciones entre ellas. Esto se hace de manera repetitiva para capturar relaciones cada vez más complejas y contextos más amplios.

\item \textbf{Decodificación}: Una vez que la entrada se ha procesado a través de las capas de atención, el modelo puede generar una salida. En el caso de los \acrshort{llm}, esta salida es una secuencia de palabras que forman una respuesta coherente a una pregunta o una generación de texto.

\item \textbf{Entrenamiento}: Los \textit{Transformers} se entrenan utilizando grandes conjuntos de datos que contienen pares de entrada y salida. El modelo ajusta sus parámetros para minimizar la diferencia entre las respuestas generadas y las respuestas esperadas en el conjunto de datos de entrenamiento.

\end{enumerate}

\imagen{transformer_model_architecture-f}{Arqitectura de los modelos de redes neuronales 
 profundas \textit{Transformers}.}{1}

\subsection{Procesamiento de datos de entrenamiento - tokenización}

Los \acrlong{llm} procesan el texto utilizando tokens, que son secuencias comunes de caracteres que se encuentran en un conjunto de texto. Los modelos aprenden a entender las relaciones estadísticas entre estos tokens y destacan a la hora de producir el siguiente token en una secuencia de tokens\cite{OpenAI_Tokenizer}.

\imagen{OpenAI_Tokenizer}{Muestra de un texto y su equivalente en tokens usando el tokenizador de OpenAI para GPT-3.5 y GPT-4.}{1}

Existen dos conceptos que están en la base de la tokenización de los \acrshort{llm}: y los N-grama y la Codificación de pares de bytes.

N-grama es una serie de n letras adyacentes (incluidos los signos de puntuación y los espacios en blanco), sílabas o, rara vez, palabras enteras que se encuentran en un conjunto de datos lingüísticos; o fonemas adyacentes extraídos de un conjunto de datos de grabación del habla, o pares de bases adyacentes extraídos de un genoma. Se recogen de un \textit{corpus} de texto o de habla. Si se utilizan prefijos numéricos latinos, el n-grama de tamaño 1 se denomina ``unigrama'', el de tamaño 2 ``bigrama'', etc. Si, en lugar de los latinos, se utilizan además los números cardinales ingleses, entonces se denominan ``four-gram'', ``five-gram'', etc. 

La codificación de pares de bytes(también conocida como codificación de digramas) es un algoritmo, descrito por primera vez en 1994 por Philip Gage\cite{Gage_Compresion}. Su modificación destaca por ser el tokenizador de modelos de lenguaje de gran tamaño con capacidad para combinar tanto tokens que codifican caracteres únicos (incluidos dígitos únicos o signos de puntuación únicos) como aquellos que codifican palabras completas (incluso las palabras compuestas más largas). Esta modificación, en el primer paso, supone que todos los caracteres únicos son un conjunto inicial de n-gramas de 1 carácter de longitud (es decir, ``tokens'' iniciales). A continuación, el par más frecuente de caracteres adyacentes se fusiona sucesivamente en un nuevo n-grama de 2 caracteres de longitud y todas las instancias del par se sustituyen por este nuevo token. Esto se repite hasta obtener un vocabulario del tamaño prescrito. Tenga en cuenta que siempre se pueden construir palabras nuevas a partir de los tokens del vocabulario final y de los caracteres del conjunto inicial.

Todos los tokens únicos encontrados en un \textit{corpus} se enumeran en un vocabulario de tokens cuyo tamaño, en el caso de GPT-3, es de 50257.

La diferencia entre el algoritmo modificado y el original es que el algoritmo original no fusiona el par más frecuente de bytes de datos, sino que los sustituye por un nuevo byte que no estaba contenido en el conjunto de datos inicial. Para reconstruir el conjunto de datos inicial se necesita una tabla de búsqueda de las sustituciones. El algoritmo es eficaz para la tokenización porque no requiere grandes gastos de cálculo y sigue siendo coherente y fiable.

\imagen{Byte-pair_Encoding}{Ejemplo de codificación de pares de bytes aplicado a una secuencia de caracteres.}{0.75}

Un vocabulario de tokens basado en las frecuencias extraídas principalmente del ingles, utiliza el menor número posible de tokens para una palabra media en ingles. Sin embargo, una palabra media en otro idioma codificada por un tokenizador optimizado para el inglés se divide en una cantidad subóptima de tokens. Como regla general, un token suele corresponder a unos 4 caracteres de texto en inglés común. Esto equivale aproximadamente a tres cuartas partes de una palabra (por tanto, 100 tokens ~= 75 palabras).

Adicionalmente a la tokenización, el procesamiento de datos intentar crear datos de entrenamiento de alta calidad. La eliminación de pasajes tóxicos del conjunto de datos, el descarte de datos de baja calidad y la desduplicación son ejemplos de limpieza de conjuntos de datos de entrenamiento. Los conjuntos de datos limpios (de alta calidad) resultantes contenían hasta 17 billones de palabras en 2022, frente a los 985 millones de palabras utilizados en 2018 para GPT-1 y los 3.300 millones de palabras utilizados para BERT. No obstante, se espera que los datos futuros estén cada vez más ``contaminados'' por los propios contenidos generados por \acrshort{llm}.

\section{Tipos de LLM}

Los \acrlong{llm} pueden clasificarse a grandes rasgos en tres tipos: modelos de preentrenamiento, modelos de \textit{Fine-tuning} y modelos multimodales\cite{scribbleData}.

\begin{description}

\item[Los modelos de preentrenamiento], como GPT-3/GPT-3.5, T5 y XLNet, se entrenan con grandes cantidades de datos, lo que les permite aprender una amplia gama de patrones y estructuras lingüísticas. Estos modelos destacan en la generación de textos coherentes y gramaticalmente correctos sobre una gran variedad de temas. Se utilizan como punto de partida para seguir entrenándolos y perfeccionándolos para tareas específicas.

\item[Los modelos de \textit{Fine-tuning}], como BERT, RoBERTa y ALBERT, se entrenan previamente en un gran conjunto de datos y luego se ajustan en un conjunto de datos más pequeño para una tarea específica. Estos modelos son muy eficaces para tareas como el análisis de sentimientos, la respuesta a preguntas y la clasificación de textos. Suelen utilizarse en aplicaciones industriales que requieren modelos lingüísticos específicos para cada tarea.

\item[Los modelos multimodales] como CLIP y DALL-E combinan texto con otras modalidades como imágenes o vídeo para crear modelos lingüísticos más robustos. Estos modelos pueden entender las relaciones entre imágenes y texto, lo que les permite generar descripciones textuales de imágenes o incluso generar imágenes a partir de descripciones textuales.

\end{description}

Cada tipo de \acrshort{llm} tiene sus puntos fuertes y débiles, y la elección de cuál utilizar depende del caso de uso específico.

\imagen{TiposLLM}{Clasificación de los LLM en tres categorias}{1}

\section{Selección de un LLM}

Aspectos a considerar: Precio, tamaño, funciones.

\subsection{OpenAI}

\imagen{OpenAI}{Evolución tecnológica de los modelos GPT}{1}

\subsection{LLaMa}

\cite{Murtuza}

\imagen{LLaMa}{Gráfico de la evolución de los modelos LLaMa}{1}

\subsection{Bard}

\subsection{Mistral}

\cite{Zhong2023AGIEvalAH}
\imagen{Mistral_Comparacion}{Resultados en MMLU, Razonamiento de sentido común, Conocimiento del mundo y Comprensión lectora para Mistral 7B y Llama 2 (7B/13/70B).}{1}

\subsection{Otros LLM}

MiniLLMs

\section{Prompt Engineering}

\section{¿Que es un \textit{Retrieval-augmented Generation}?}

Los LLM han demostrado su capacidad para comprender el contexto y ofrecer respuestas precisas a diversas tareas de procesamiento de lenguaje natural (PLN), como la síntesis o las preguntas y respuestas, cuando se les solicita. Aunque son capaces de ofrecer muy buenas respuestas a preguntas sobre información con la que fueron entrenados, tienden a ``alucinar'' cuando el tema trata sobre información que desconocen, es decir, que no estaba incluida en sus datos de entrenamiento.

``\textit{Retrieval-augmented Generation}'' (RAG) es una técnica avanzada en el campo del PNL que combina dos enfoques clave: recuperación y generación de texto. Esta técnica se utiliza para mejorar la generación de texto automática y garantizar que las respuestas generadas sean precisas, relevantes y contextualmente adecuadas\cite{Lewis2020}.

En un sistema de RAG, el proceso se divide en dos etapas:

\begin{enumerate}
    \item Recuperación (\textit{Retrieval}): En esta etapa, el sistema busca información relevante en grandes conjuntos de datos o bases de conocimiento. Utiliza métodos de recuperación de información para encontrar documentos o fragmentos de texto que contienen información relacionada con la consulta o el contexto actual.

    \item Generación (\textit{Generation}): Una vez que se ha recuperado la información relevante, el sistema de generación de texto (a menudo basado en un LLM, como GPT) utiliza esta información para generar respuestas coherentes y contextualmente apropiadas.
    
\end{enumerate}

La combinación de estas dos etapas permite que la técnica RAG proporcione respuestas que no solo se basen en el conocimiento preexistente\cite{chen-etal-2017-reading}, sino que también sean sensibles al contexto específico de la consulta o la tarea. Esto propicia respuestas más precisas y relevantes en comparación con enfoques puramente generativos\cite{fan-etal-2019-eli5,hossain-etal-2020-simple}.

RAG se utiliza en una variedad de aplicaciones, incluyendo chatbots, sistemas de respuesta automática, motores de búsqueda mejorados y generación de contenido automático, donde la capacidad de acceder y utilizar información específica es esencial para brindar respuestas mas precisas.

\section{Embeddings}

\section{Bases de datos Vectoriales}

\section{Tablas}

Igualmente se pueden usar los comandos específicos de \LaTeX o bien usar alguno de los comandos de la plantilla.

\tablaSmall{Herramientas y tecnologías utilizadas en cada parte del proyecto}{l c c c c}{herramientasportipodeuso}
{ \multicolumn{1}{l}{Herramientas} & App AngularJS & API REST & BD & Memoria \\}{ 
HTML5 & X & & &\\
CSS3 & X & & &\\
BOOTSTRAP & X & & &\\
JavaScript & X & & &\\
AngularJS & X & & &\\
Bower & X & & &\\
PHP & & X & &\\
Karma + Jasmine & X & & &\\
Slim framework & & X & &\\
Idiorm & & X & &\\
Composer & & X & &\\
JSON & X & X & &\\
PhpStorm & X & X & &\\
MySQL & & & X &\\
PhpMyAdmin & & & X &\\
Git + BitBucket & X & X & X & X\\
Mik\TeX{} & & & & X\\
\TeX{}Maker & & & & X\\
Astah & & & & X\\
Balsamiq Mockups & X & & &\\
VersionOne & X & X & X & X\\
} 
