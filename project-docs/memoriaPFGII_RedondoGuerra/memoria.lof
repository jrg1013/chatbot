\babel@toc {spanish}{}\relax 
\babel@toc {spanish}{}\relax 
\contentsline {figure}{\numberline {3.1}{\ignorespaces Cronología de la evolución de los LLM}}{8}{figure.3.1}%
\contentsline {figure}{\numberline {3.2}{\ignorespaces Cronología de los modelos lingüísticos de gran tamaño (superior a 10B) de los últimos años}}{11}{figure.3.2}%
\contentsline {figure}{\numberline {3.3}{\ignorespaces Ilustración de un espacio de incrustación 2D tal que el vector que une ``lobo'' con ``perro'' es el mismo que el vector que une ``tigre'' con ``gato''.}}{12}{figure.3.3}%
\contentsline {figure}{\numberline {3.4}{\ignorespaces Arquitectura de los modelos de redes neuronales profundas \textit {Transformers}.}}{15}{figure.3.4}%
\contentsline {figure}{\numberline {3.5}{\ignorespaces Muestra de un texto y su equivalente en tokens usando el tokenizador de OpenAI para GPT-3.5 y GPT-4.}}{16}{figure.3.5}%
\contentsline {figure}{\numberline {3.6}{\ignorespaces Ejemplo de codificación de pares de bytes aplicado a una secuencia de caracteres.}}{18}{figure.3.6}%
\contentsline {figure}{\numberline {3.7}{\ignorespaces Cómo funciona self-attention: se calculan las puntuaciones de atención entre ``estación'' y cualquier otra palabra de la secuencia, y luego se utilizan para ponderar una suma de vectores de palabras que se convierte en el nuevo vector ``estación''.}}{20}{figure.3.7}%
\contentsline {figure}{\numberline {3.8}{\ignorespaces Clasificación de los LLM en tres categorías}}{22}{figure.3.8}%
\contentsline {figure}{\numberline {3.9}{\ignorespaces Evolución tecnológica de los modelos GPT a lo largo de los años.}}{23}{figure.3.9}%
\contentsline {figure}{\numberline {3.10}{\ignorespaces Gráfico de la evolución de los modelos LLaMa}}{24}{figure.3.10}%
\contentsline {figure}{\numberline {3.11}{\ignorespaces Resultados en MMLU, Razonamiento de sentido común, Conocimiento del mundo y Comprensión lectora para Mistral 7B y Llama 2 (7B/13/70B).}}{28}{figure.3.11}%
\contentsline {figure}{\numberline {3.12}{\ignorespaces Secuencia para la creación de un \textit {Retrieval-augmented Generation}.}}{33}{figure.3.12}%
\contentsline {figure}{\numberline {3.13}{\ignorespaces Esquema del funcionamiento de un \textit {Retrieval-augmented Generation}.}}{33}{figure.3.13}%
\contentsline {figure}{\numberline {3.14}{\ignorespaces Esquema del funcionamiento de creación de bases de datos vectoriales con Embeddings.}}{37}{figure.3.14}%
\contentsline {figure}{\numberline {3.15}{\ignorespaces Proceso de gestión de TFG en la UBU.}}{41}{figure.3.15}%
\contentsline {figure}{\numberline {3.16}{\ignorespaces UI del UBU-CHATBOT actual.}}{47}{figure.3.16}%
\contentsline {figure}{\numberline {3.17}{\ignorespaces Limitaciones del UBU-CHATBOT actual relativas a la formulación de preguntas por parte de los estudiantes.}}{48}{figure.3.17}%
\contentsline {figure}{\numberline {4.1}{\ignorespaces Esquema del \textit {Framework} de Langchain con los distintos componentes y módulos.}}{52}{figure.4.1}%
\contentsline {figure}{\numberline {4.2}{\ignorespaces Matriz de representación de los múltiples \textit {Document Loaders} disponibles en LangChain.}}{55}{figure.4.2}%
\contentsline {figure}{\numberline {4.3}{\ignorespaces Efecto de la \textit {Quantization} en el tamaño de los LLM de LLaMa.}}{58}{figure.4.3}%
\contentsline {figure}{\numberline {4.4}{\ignorespaces \textit {Sprint Board} de Zube.io con los distintos estados de las \textit {Issues}.}}{64}{figure.4.4}%
\contentsline {figure}{\numberline {5.1}{\ignorespaces Avances en el campo de la Inteligencia Artificial Generativa en los últimos meses.}}{76}{figure.5.1}%
\contentsline {figure}{\numberline {5.2}{\ignorespaces Ejemplo de reporte de testeo de una posible configuración del RAG.}}{83}{figure.5.2}%
\contentsline {figure}{\numberline {5.3}{\ignorespaces Ejemplo de la validación de Preguntas y Respuestas del chatbot y su respuesta generada.}}{84}{figure.5.3}%
\contentsline {figure}{\numberline {6.1}{\ignorespaces Ejemplo de pregunta de tutoría administrativa, saludo, del chatbot de DialogFlow.}}{89}{figure.6.1}%
\contentsline {figure}{\numberline {6.2}{\ignorespaces Ejemplo de pregunta de tutoría administrativa, saludo, del chatbot basado en LLM y RAG.}}{89}{figure.6.2}%
\contentsline {figure}{\numberline {6.3}{\ignorespaces Ejemplo de pregunta de tutoría administrativa, profesores del tribunal, del chatbot de DialogFlow.}}{90}{figure.6.3}%
\contentsline {figure}{\numberline {6.4}{\ignorespaces Ejemplo de pregunta de tutoría administrativa, profesores del tribunal, del chatbot basado en LLM y RAG.}}{91}{figure.6.4}%
\contentsline {figure}{\numberline {6.5}{\ignorespaces Ejemplo de pregunta de tutoría administrativa, profesores del tribunal pregunta compleja, del chatbot de DialogFlow.}}{92}{figure.6.5}%
\contentsline {figure}{\numberline {6.6}{\ignorespaces Ejemplo de pregunta de tutoría administrativa, profesores del tribunal pregunta compleja, del chatbot basado en LLM y RAG.}}{92}{figure.6.6}%
\contentsline {figure}{\numberline {6.7}{\ignorespaces Ejemplo de pregunta de tutoría general del chatbot basado en LLM y RAG.}}{93}{figure.6.7}%
\contentsline {figure}{\numberline {6.8}{\ignorespaces Ejemplo de pregunta de tutoría general del chatbot basado en LLM y RAG.}}{94}{figure.6.8}%
\contentsline {figure}{\numberline {6.9}{\ignorespaces Ejemplo de pregunta de tutoría general del chatbot basado en LLM y RAG.}}{95}{figure.6.9}%
\contentsline {figure}{\numberline {6.10}{\ignorespaces Ejemplo de pregunta de tutoría especificas del chatbot basado en LLM y RAG.}}{96}{figure.6.10}%
