@inproceedings{Lewis2020,
 author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K\"{u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt\"{a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {9459--9474},
 publisher = {Curran Associates, Inc.},
 title = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{lopez2009proceso,
title={Proceso de gestión de Trabajos Fin de Carrera},
author={López Nozal, Carlos and Marticorena Sánchez, Raúl and Rodríguez, Juan José and Bustillo Iglesias, Andrés},
booktitle={Jornadas de Enseñanza Universitaria de la Informática},
pages={413--420},
year={2009},
organization={Jornadas de Enseñanza Universitaria de la Informática}
}

@inproceedings{lopez2010final,
title={Final year project management process},
author={López, Carlos and Martín, David H and Bustillo, Andrés and Marticorena, Raúl},
booktitle={Proceedings 2nd International Conference on Computer Supported Education},
volume={2},
pages={5--12},
year={2010}
}

@article{Mikolov2013Word2Vec,
  added-at = {2013-10-23T23:02:12.000+0200},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  biburl = {https://www.bibsonomy.org/bibtex/29665b85e8756834ac29fcbd2c6ad0837/wool},
  ee = {http://arxiv.org/abs/1301.3781},
  interhash = {e92df552b17e9f952226a893b84ad739},
  intrahash = {9665b85e8756834ac29fcbd2c6ad0837},
  journal = {CoRR},
  keywords = {nlp},
  timestamp = {2013-10-23T23:02:12.000+0200},
  title = {Efficient Estimation of Word Representations in Vector Space},
  url = {http://dblp.uni-trier.de/db/journals/corr/corr1301.html#abs-1301-3781},
  volume = {abs/1301.3781},
  year = 2013
}

@misc{zhao2023survey,
  abstract = {Language is essentially a complex, intricate system of human expressions
governed by grammatical rules. It poses a significant challenge to develop
capable AI algorithms for comprehending and grasping a language. As a major
approach, language modeling has been widely studied for language understanding
and generation in the past two decades, evolving from statistical language
models to neural language models. Recently, pre-trained language models (PLMs)
have been proposed by pre-training Transformer models over large-scale corpora,
showing strong capabilities in solving various NLP tasks. Since researchers
have found that model scaling can lead to performance improvement, they further
study the scaling effect by increasing the model size to an even larger size.
Interestingly, when the parameter scale exceeds a certain level, these enlarged
language models not only achieve a significant performance improvement but also
show some special abilities that are not present in small-scale language
models. To discriminate the difference in parameter scale, the research
community has coined the term large language models (LLM) for the PLMs of
significant size. Recently, the research on LLMs has been largely advanced by
both academia and industry, and a remarkable progress is the launch of ChatGPT,
which has attracted widespread attention from society. The technical evolution
of LLMs has been making an important impact on the entire AI community, which
would revolutionize the way how we develop and use AI algorithms. In this
survey, we review the recent advances of LLMs by introducing the background,
key findings, and mainstream techniques. In particular, we focus on four major
aspects of LLMs, namely pre-training, adaptation tuning, utilization, and
capacity evaluation. Besides, we also summarize the available resources for
developing LLMs and discuss the remaining issues for future directions.},
  added-at = {2023-07-17T05:22:59.000+0200},
  author = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du, Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen, Ji-Rong},
  biburl = {https://www.bibsonomy.org/bibtex/2e99e12e8ea5ae0f885fa255373e25761/desplode},
  description = {A Survey of Large Language Models},
  interhash = {68ff0196340f5c8c03b2a348436a41f3},
  intrahash = {e99e12e8ea5ae0f885fa255373e25761},
  keywords = {llm},
  note = {cite arxiv:2303.18223Comment: ongoing work; 85 pages, 610 citations},
  timestamp = {2023-07-23T10:57:33.000+0200},
  title = {A Survey of Large Language Models},
  url = {http://arxiv.org/abs/2303.18223},
  year = 2023
}

@online{scribbleData,
    author = "ScribbleData",
    title = "Large Language Models 101: History, Evolution and Future",
    url  = "https://www.scribbledata.io/large-language-models-history-evolutions-and-future/",
    addendum = "(accessed: 26.10.2023)",
    keywords = "LLM,history"
}

@online{Tolaka,
    author = "Toloka Team",
    title = "The history, timeline, and future of LLMs",
    url  = "https://toloka.ai/blog/history-of-llms",
    addendum = "(accessed: 26.10.2023)",
    keywords = "LLM,history"
}

@inproceedings{chen-etal-2017-reading,
    title = "Reading {W}ikipedia to Answer Open-Domain Questions",
    author = "Chen, Danqi  and
      Fisch, Adam  and
      Weston, Jason  and
      Bordes, Antoine",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1171",
    doi = "10.18653/v1/P17-1171",
    pages = "1870--1879",
    abstract = "This paper proposes to tackle open-domain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval (finding the relevant articles) with that of machine comprehension of text (identifying the answer spans from those articles). Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task.",
}

@inproceedings{fan-etal-2019-eli5,
    title = "{ELI}5: Long Form Question Answering",
    author = "Fan, Angela  and
      Jernite, Yacine  and
      Perez, Ethan  and
      Grangier, David  and
      Weston, Jason  and
      Auli, Michael",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1346",
    doi = "10.18653/v1/P19-1346",
    pages = "3558--3567",
    abstract = "We introduce the first large-scale corpus for long form question answering, a task requiring elaborate and in-depth answers to open-ended questions. The dataset comprises 270K threads from the Reddit forum {``}Explain Like I{'}m Five{''} (ELI5) where an online community provides answers to questions which are comprehensible by five year olds. Compared to existing datasets, ELI5 comprises diverse questions requiring multi-sentence answers. We provide a large set of web documents to help answer the question. Automatic and human evaluations show that an abstractive model trained with a multi-task objective outperforms conventional Seq2Seq, language modeling, as well as a strong extractive baseline. However, our best model is still far from human performance since raters prefer gold responses in over 86{\%} of cases, leaving ample opportunity for future improvement.",
}

@inproceedings{hossain-etal-2020-simple,
    title = "Simple and Effective Retrieve-Edit-Rerank Text Generation",
    author = "Hossain, Nabil  and
      Ghazvininejad, Marjan  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.228",
    doi = "10.18653/v1/2020.acl-main.228",
    pages = "2532--2538",
    abstract = "Retrieve-and-edit seq2seq methods typically retrieve an output from the training set and learn a model to edit it to produce the final output. We propose to extend this framework with a simple and effective post-generation ranking approach. Our framework (i) retrieves several potentially relevant outputs for each input, (ii) edits each candidate independently, and (iii) re-ranks the edited candidates to select the final output. We use a standard editing model with simple task-specific re-ranking approaches, and we show empirically that this approach outperforms existing, significantly more complex methodologies. Experiments on two machine translation (MT) datasets show new state-of-art results. We also achieve near state-of-art performance on the Gigaword summarization dataset, where our analyses show that there is significant room for performance improvement with better candidate output selection in future work.",
}

@online{Chollet,
    author = "Francois Chollet",
    title = "How I think about LLM prompt engineering",
    url  = "https://fchollet.substack.com/p/how-i-think-about-llm-prompt-engineering",
    addendum = "(accessed: 23.10.2023)",
    keywords = "LLM,prompt,Embeddings,VectorDB"
}

@online{Murtuza,
    author = "Murtuza Kazmi",
    title = "Using LLaMA 2.0, FAISS and LangChain for Question-Answering on Your Own Data",
    url  = "https://medium.com/@murtuza753/using-llama-2-0-faiss-and-langchain-for-question-answering-on-your-own-data-682241488476",
    addendum = "(accessed: 01.11.2023)",
    keywords = "LLM,LLaMa2.0,FAISS,LangChain"
}

@article{Zhong2023AGIEvalAH,
  title={AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models},
  author={Wanjun Zhong and Ruixiang Cui and Yiduo Guo and Yaobo Liang and Shuai Lu and Yanlin Wang and Amin Saied Sanosi Saied and Weizhu Chen and Nan Duan},
  journal={ArXiv},
  year={2023},
  volume={abs/2304.06364},
  url={https://api.semanticscholar.org/CorpusID:258108259}
}

@misc{radford_language_2019,
  abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
  added-at = {2023-01-14T15:28:29.000+0100},
  author = {Radford, Alec and Wu, Jeff and Child, Rewon and Luan, D. and Amodei, Dario and Sutskever, Ilya},
  biburl = {https://www.bibsonomy.org/bibtex/272c31587e067e0041527dabb3a34cdb8/lepsky},
  interhash = {b926ece39c03cdf5499f6540cf63babd},
  intrahash = {72c31587e067e0041527dabb3a34cdb8},
  keywords = {chatgpt kuenstliche_intelligenz},
  timestamp = {2023-01-14T15:33:48.000+0100},
  title = {Language models are unsupervised multitask learners},
  url = {https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe},
  urldate = {2023-01-06},
  year = 2019
}

@online{Nvidia_Transformers,
    author = "Rick Merritt",
    title = "What Is a Transformer Model?",
    url  = "https://blogs.nvidia.com/blog/2022/03/25/what-is-a-transformer-model/",
    addendum = "(accessed: 03.11.2023)",
    keywords = "LLM,Transformers,DNN"
}

@article{bowman2023eight,
  title={Eight things to know about large language models},
  author={Bowman, Samuel R},
  journal={arXiv preprint arXiv:2304.00612},
  year={2023}
}

@online{OpenAI_Tokenizer,
    author = "OpenAI",
    title = "Tokenizer",
    url  = "https://platform.openai.com/tokenizer",
    addendum = "(accessed: 03.11.2023)",
    keywords = "LLM,Transformers,Tokenizer"
}

@online{Gage_Compresion,
    author = "Philip Gage",
    title = "A New Algorithm for Data Compression",
    url  = "http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM",
    addendum = "(accessed: 03.11.2023)",
    keywords = "LLM,Compression,Tokens"
}

@online{Techtarget_LLM,
    author = "Sean Michael Kerner",
    title = "Large language models (LLMs)",
    url  = "https://www.techtarget.com/whatis/definition/large-language-model-LLM",
    addendum = "(accessed: 03.11.2023)",
    keywords = "LLM,Transformers,Tokens"
}

@online{LlamaIndex,
    author = "LlamaIndex",
    title = "Using LLMs",
    url  = "https://docs.llamaindex.ai/en/stable/module_guides/models/llms.html",
    addendum = "(accessed: 03.11.2023)",
    keywords = "LLM, Open Source"
}

@misc{turner2023introduction,
      title={An Introduction to Transformers}, 
      author={Richard E. Turner},
      year={2023},
      eprint={2304.10557},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@online{MindsDB,
    author = "MindsDB",
    title = "Navigating the LLM Landscape: A Comparative Analysis of Leading Large Language Models",
    url  = "https://dev.to/mindsdb/navigating-the-llm-landscape-a-comparative-analysis-of-leading-large-language-models-1ocn",
    addendum = "(accessed: 12.11.2023)",
    keywords = "LLM"
}

@online{Hostinger,
    author = "Akshay K",
    title = "Best Large Language Models for 2023 and How to Choose the Right One for Your Site",
    url  = "https://www.hostinger.com/tutorials/large-language-models",
    addendum = "(accessed: 13.11.2023)",
    keywords = "LLM, Price, Comparison"
}

@online{Anthropic,
    author = "Anthropic",
    title = "Introducing Claude",
    url  = "https://www.anthropic.com/index/introducing-claude",
    addendum = "(accessed: 10.11.2023)",
    keywords = "LLM, Claude, Anthopic"
}

@online{MiitaMistral,
    author = "Aayush Mitta",
    title = "Mistral AI: Setting New Benchmarks Beyond Llama2 in the Open-Source Space",
    url  = "https://www.unite.ai/mistral-7b-setting-new-benchmarks-beyond-llama2-in-the-open-source-space/",
    addendum = "(accessed: 13.11.2023)",
    keywords = "LLM, Mistral, LLaMa"
}

@online{LangchainRAG,
    author = "Langchain",
    title = "Query Transformations",
    url  = "https://blog.langchain.dev/query-transformations//",
    addendum = "(accessed: 13.11.2023)",
    keywords = "LLM, RAG, VectorDB"
}

@misc{wei2023chainofthought,
      title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models}, 
      author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
      year={2023},
      eprint={2201.11903},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{mikolov2013distributed,
      title={Distributed Representations of Words and Phrases and their Compositionality}, 
      author={Tomas Mikolov and Ilya Sutskever and Kai Chen and Greg Corrado and Jeffrey Dean},
      year={2013},
      eprint={1310.4546},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@online{MicrosoftVectorDatabase,
    author = "Microsoft",
    title = "What is a vector database?",
    url  = "https://learn.microsoft.com/en-us/semantic-kernel/memories/vector-db",
    addendum = "(accessed: 14.11.2023)",
    keywords = "VectorDB, Embedding"
}

@online{GGML_Gimmi,
    author = "Phillip Gimmi",
    title = "What is GGUF and GGML?",
    url  = "https://medium.com/@phillipgimmi/what-is-gguf-and-ggml-e364834d241c",
    addendum = "(accessed: 21.11.2023)",
    keywords = "GGML, Qunatization"
}

@online{UBU-Chatbot,
    author = "Alfredo Asensio Vázquez",
    title = "Desarrollo y explotación de un chatbot de FAQs sobre una asignatura de Trabajo Fin de Grado",
    url  = "https://github.com/aav0038/UBU-CHATBOT/tree/master",
    addendum = "(accessed: 01.12.2023)",
    keywords = "Chatbot, TFG, UBU"
}

@online{Streamlit,
    author = "Streamlit",
    title = "Get started",
    url  = "https://docs.streamlit.io/library/get-started",
    addendum = "(accessed: 12.12.2023)",
    keywords = "UI, Python, Data"
}

@online{HuggingFace,
    author = "Hugging Face",
    title = "About Hugging Face",
    url  = "https://huggingface.co/huggingface",
    addendum = "(accessed: 12.12.2023)",
    keywords = "Models, Data, LLMs"
}

@online{Faiss,
    author = "Hervé Jegou, Matthijs Douze, Jeff Johnson",
    title = "Faiss: A library for efficient similarity search",
    url  = "https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/
",
    addendum = "(accessed: 19.12.2023)",
    keywords = "Vector Database, Similarity Search, LLMs"
}

@online{Langchain,
    author = "Introduction",
    title = "Introduction to Langchain",
    url  ="https://python.langchain.com/docs/get_started/introduction",
    addendum = "(accessed: 18.12.2023)",
    keywords = "Langchain, Data, LLMs"
}

@online{llama.cpp,
    author = "Georgi Gerganov",
    title = "llama.cpp",
    url  ="https://github.com/ggerganov/llama.cpp",
    addendum = "(accessed: 19.12.2023)",
    keywords = "llama.cpp, LLMs"
}

@online{gpt4all,
    author = "Nomic",
    title = "gpt4all",
    url  ="https://gpt4all.io/index.html",
    addendum = "(accessed: 19.12.2023)",
    keywords = "gpt4all, LLMs"
}

@online{Ǎguila,
    author = "Mapama247",
    title = "Introducing Ǎguila, a new open-source LLM for Spanish and Catalan",
    url  ="https://medium.com/@mpamies247/introducing-a%CC%8Cguila-a-new-open-source-llm-for-spanish-and-catalan-ee1ebc70bc79",
    addendum = "(accessed: 22.12.2023)",
    keywords = "Espanol, LLMs"
}

@misc{wei2023polylm,
      title={PolyLM: An Open Source Polyglot Large Language Model}, 
      author={Xiangpeng Wei and Haoran Wei and Huan Lin and Tianhao Li and Pei Zhang and Xingzhang Ren and Mei Li and Yu Wan and Zhiwei Cao and Binbin Xie and Tianxiang Hu and Shangjie Li and Binyuan Hui and Bowen Yu and Dayiheng Liu and Baosong Yang and Fei Huang and Jun Xie},
      year={2023},
      eprint={2307.06018},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@online{ubu-tfg,
    author = "UBU",
    title = "Reglamento sobre trabajo fin de grado y trabajo fin de master",
    url  ="https://www.ubu.es/sites/default/files/portal_page/files/reglamentp_tfg-tfm_aprob._08-06-2022.pdf",
    addendum = "(accessed: 04.01.2024)",
    keywords = "UBU, TFG"
}

@misc{ ChatbotGrowth,
	author = "Markets and Markets",
	title = "Conversational AI Market",
	year = "2021",
	url = {https://www.marketsandmarkets.com/Market-Reports/conversational-ai-market-49043506.html},
	note = "[Online; Accedido 17-junio-2021]"
}

@misc{ UBUAssistant,
	author = "Daniel Santidrián Alonso",
	title = "UBUAssistant",
	year = "2018",
	url = {https://github.com/DanielSantidrian/UBUassistant},
	note = "[Online; Accedido 19-junio-2021]"
}

@misc{ ChatbotTourist,
	author = "Jasmin Wellnitz",
	title = "Development of a chatbot for tourist recommendations",
	year = "2017",
	url = {https://github.com/jaswellnitz/tourist-chatbot},
	note = "[Online; Accedido 19-junio-2021]"
}

@misc{ UBUVoiceAssistant,
	author = "Álvaro Delgado Pascual",
	title = "UBUVoiceAssistant",
	year = "2020",
	url = {https://github.com/adp1002/UBUVoiceAssistant},
	note = "[Online; Accedido 19-junio-2021]"
} 

@misc{ repositorioGithub,
	author = "Alfredo Asensio Vázquez",
	title = "Repositorio UBUChatbot",
	year = "2021",
	url = {https://github.com/aav0038/CHATBOT_TFG},
	note = "[Online; Accedido 23-junio-2021]"
} 
@misc{ repositorioGithub,
	author = "Miguel Collado",
	title = "Repositorio Teachbot Practicum",
	year = "2023",
	url = {https://gitlab.com/tfg6300404/chatbob-practicum},
	note = "[Online; Accedido 23-junio-2021]"
} 

@online{LangChainNot,
    author = "Alden Do Rosario",
    title = "Langchain is NOT for production use. Here is why ..",
    url  ="https://medium.com/@aldendorosario/langchain-is-not-for-production-use-here-is-why-9f1eca6cce80",
    addendum = "(accessed: 05.01.2024)",
    keywords = "Langchain, LLMs"
}

@misc{schäfer2023empirical,
      title={An Empirical Evaluation of Using Large Language Models for Automated Unit Test Generation}, 
      author={Max Schäfer and Sarah Nadi and Aryaz Eghbali and Frank Tip},
      year={2023},
      eprint={2302.06527},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@online{Low-shot,
    author = "Jason Brownlee",
    title = "Prompt Engineering for Effective Interaction with ChatGPT",
    url  ="https://machinelearningmastery.com/prompt-engineering-for-effective-interaction-with-chatgpt/",
    addendum = "(accessed: 07.01.2024)",
    keywords = "LLMs"
}

@online{digit,
    author = "DIGIT - Grupo de innovación docente de la Universidad de Burgos",
    title = "Consultando a chatGPT sobre Trabajos Fin de Grado",
    url  ="https://digit-moodle.blogspot.com/2023/03/consultando-chatgpt-sobre-trabajos-fin.html",
    addendum = "(accessed: 11.01.2024)",
    keywords = "ChatGPT, TFG"
}

@online{DeepLearning,
    author = "Harrison Chase",
    title = "LangChain for LLM Application Development",
    url  ="https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/",
    addendum = "(accessed: 10.10.2023)",
    keywords = "LLM, LangChain"
}

@misc{barnett2024seven,
      title={Seven Failure Points When Engineering a Retrieval Augmented Generation System}, 
      author={Scott Barnett and Stefanus Kurniawan and Srikanth Thudumu and Zach Brannelly and Mohamed Abdelrazek},
      year={2024},
      eprint={2401.05856},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}